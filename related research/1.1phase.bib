@inproceedings{Alhadreti2016,
abstract = {Usability has become an imperative aspect of survival on the web, thus, it has always been considered as a crucial aspect of web design. This paper presents the results of a study that compared two think-aloud usability testing methods: the concurrent think-aloud and the retrospective think-aloud methods. Data from task performance, testing experience, and usability problems were collected from 40 participants equally distributed between the two think-aloud conditions. The results found that while the thinking aloud method had no impact on task performance and participants testing experience, participants using the concurrent think-aloud method detected a larger number of minor problems with the test interface than participants using the retrospective think-aloud method. These findings suggest a reason for preferring the concurrent think-aloud method to the retrospective one.},
author = {Alhadreti, Obead and Mayhew, Pam},
booktitle = {Proceedings of the 30th International BCS Human Computer Interaction Conference, HCI 2016},
doi = {10.14236/ewic/hci2016.101},
keywords = {Think-aloud protocols,Usability testing,User studies},
title = {{“Thinking about thinking aloud”: An investigation of think-aloud methods in usability testing}},
url = {http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.699593},
volume = {2016-July},
year = {2016}
}
@inproceedings{Chaim2003,
abstract = {Testing and debugging are activities that consume a significant amount of the software quality and maintenance budget. To reduce such cost, the use of testing information for debugging purposes has been advocated In general, heuristics are used to select structural testing requirements (nodes, branches, and definition use associations) more closely related to the manifestation of failures which are then mapped into pieces of code. The problem with this approach is that it relies only on a static information-a fragment of code. We introduce a strategy for fault localization based on the investigation of indications (or hints) provided at run-time by data-flow testing requirements. The intuition is that the selected data-flow testing requirements may fail to hit the fault site but they still provide useful information for fault localization. The strategy-called Debugging strategy based on Requirements of Testing (DRT)-is targeted to such situations. The strategy's novelty and attractiveness are threefold: (i) it focuses on dynamic information related to testing data; (ii) it can be implemented in state-of-the-practice symbolic debuggers with limited overhead; and (iii) it utilizes algorithms that consume constant memory and are linear with respect to the number of branches in the program. The paper presents a case study which shows that our intuition is valid (for the subject program) and a proof-of-concept tool that implements the strategy. {\textcopyright} 2003 IEEE.},
author = {Chaim, M. L. and Maldonado, J. C. and Jino, M.},
booktitle = {Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
doi = {10.1109/CSMR.2003.1192424},
isbn = {0769519024},
issn = {15345351},
pages = {160--169},
title = {{A debugging strategy based on requirements of testing}},
year = {2003}
}
@inproceedings{Chaim2004,
abstract = {Testing and debugging activities consume a significant amount of the software development and maintenance budget. To reduce this cost, the use of testing information for debugging purposes has been advocated. In general, heuristics are used to select structural testing requirements (nodes, branches and definition-use associations) more closely related to the manifestation of a failure, which are then mapped into a piece of code. The intuition is that the selected piece of code is likely to contain the fault. However, this approach has its drawbacks. Heuristics that select a manageable piece of code are less likely to hit the fault and the piece of code itself does not provide enough guidance for program understanding - a major factor in program debugging. These problems occur because this approach relies only on static Information - a fragment of code. We introduce a strategy for fault localization that addresses these problems. The strategy - called the debugging strategy based on the requirements of testing (DRT) - is based on the investigation of indications (or hints) provided at run-time by data-flow testing requirements (definition-use associations). Our claim is that the selected definition-use associations may fail to hit the fault site, but still provide information useful for fault localization. The strategy's novelty and attractiveness are threefold: (i) the focus on dynamic information related to testing data; (ii) implementation in state-of-the-practice symbolic debuggers with a low overhead; and (iii) the use of algorithms which consume constant memory and are linear on the number of branches in the program. A case study shows that our claim is valid (for the subject program) and a prototype tool implements the strategy. Copyright {\textcopyright} 2004 John Wiley & Sons, Ltd.},
author = {Chaim, Marcos L. and Maldonado, Jos{\'{e}} C. and Jino, Mario},
booktitle = {Journal of Software Maintenance and Evolution},
doi = {10.1002/smr.297},
issn = {1532060X},
keywords = {Automated debugging,Data-flow testing,Debugging tool,Dynamic testing information,Fault localization},
number = {4-5},
pages = {277--308},
publisher = {John Wiley and Sons Ltd},
title = {{A debugging strategy based on the requirements of testing}},
volume = {16},
year = {2004}
}
@article{Lin2021,
author = {Lin, Chu-Ti and Chen, Wen-Yuan and Intasara, Jutarporn},
doi = {10.1109/access.2021.3086878},
file = {::},
journal = {IEEE Access},
month = {jun},
pages = {82577--82596},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{A Framework for Improving Fault Localization Effectiveness Based on Fuzzy Expert System}},
volume = {9},
year = {2021}
}
@article{Wang2021,
abstract = {Coverage-Based Fault Localization (CBFL) techniques are based on the conjecture that the program elements executed by more failed test cases and less passed test cases have more chance to be faulty. Coincidental Correct (CC) test case is one of the negative impacts on CBFL, for the reason that the CC test cases execute the faulty element but not propagate the faulty status to the final output. To alleviate the negative impact of CC test cases, in this paper, we propose a cluster-based technique to identify CC test cases from the passed test suite. To evaluate the effectiveness of our method, we conduct empirical studies on 102 versions from six programs. The experimental results show that, when using our method, it can accurately recognize CC test cases, where the precision and recall rate are both higher than 85%. A further study shows that, after removing identified CC test cases, the fault localization accuracy of SBFL can be improved apparently.},
author = {Wang, Weibo and Wu, Yonghao and Liu, Yong},
doi = {10.1142/S0218126621500535},
issn = {02181266},
journal = {Journal of Circuits, Systems and Computers},
keywords = {Fault localization,K-means cluster,coincidental correctness},
month = {mar},
number = {3},
pages = {2150053:1--2150053:19},
publisher = {World Scientific},
title = {{A Passed Test Case Cluster Method to Improve Fault Localization}},
volume = {30},
year = {2021}
}
@article{Kashiwa2020,
abstract = {This paper proposes a release-aware bug triaging method that aims to increase the number of bugs that developers can fix by the next release date during open-source software development. A variety of methods have been proposed for recommending appropriate developers for particular bug-fixing tasks, but since these approaches only consider the developers' ability to fix the bug, they tend to assign many of the bugs to a small number of the project's developers. Since projects generally have a release schedule, even excellent developers cannot fix all the bugs that are assigned to them by the existing methods. The proposed method places an upper limit on the number of tasks which are assigned to each developer during a given period, in addition to considering the ability of developers. Our method regards the bug assignment problem as a multiple knapsack problem, finding the best combination of bugs and developers. The best combination is one that maximizes the efficiency of the project, while meeting the constraint where it can only assign as many bugs as the developers can fix during a given period. We conduct the case study, applying our method to bug reports from Mozilla Firefox, Eclipse Platform and GNU compiler collection (GCC). We find that our method has the following properties: (1) it can prevent the bug-fixing load from being concentrated on a small number of developers; (2) compared with the existing methods, the proposed method can assign a more appropriate amount of bugs that each developer can fix by the next release date; (3) it can reduce the time taken to fix bugs by 35%–41%, compared with manual bug triaging.},
author = {Kashiwa, Yutaro and Ohira, Masao},
doi = {10.1587/transinf.2019EDP7152},
file = {::},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Bug triage,Machine learning,Optimization,Project management,Repository mining},
number = {2},
pages = {348--362},
publisher = {Institute of Electronics, Information and Communication, Engineers, IEICE},
title = {{A release-aware bug triaging method considering developers' bug-fixing loads}},
volume = {E103D},
year = {2020}
}
@article{Etemadi2021,
abstract = {The efficient assignment of bug fixing tasks to software developers is of major importance in software maintenance and evolution. When those tasks are not efficiently assigned to developers, the software project might confront extra costs and delays. In this paper, we propose a strategy that minimizes the time and the cost in bug fixing by finding the best feasible developer arrangement to handle bug fixing requests. We enhance therefore a state-of-the-art solution that uses an evolutionary bi-objective algorithm by involving a scheduling-driven approach that explores more parts of the search space. Scheduling is the process of evaluating all possible orders that developers can follow to fix the bugs they have been assigned. Through an empirical study we analyze the performance of the scheduling-driven approach and compare it to state of the art solutions. A non-parametric statistical test with four quality indicator metrics is used to assure its superiority. The experiments using two case-studies (JDT and Platform) showed that the scheduling-driven approach is superior to the state of the art approach in 71% and 74% of cases, respectively. Thus, our approach offers superior performance by assigning more conveniently bug fixing tasks to developers, while still avoiding to overload developers.},
author = {Etemadi, Vahid and Bushehrian, Omid and Akbari, Reza and Robles, Gregorio},
doi = {10.1016/j.jss.2021.110967},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Bug assignment,Bug fixing,Scheduling-driven,Software maintenance},
month = {aug},
pages = {110967},
publisher = {Elsevier Inc.},
title = {{A scheduling-driven approach to efficiently assign bug fixing tasks to developers}},
volume = {178},
year = {2021}
}
@inproceedings{Grigoreanu2010,
abstract = {End-user programmers' code is notoriously buggy. This problem is amplified by the increasing complexity of end users' programs. To help end users catch errors early and reliably, we employ a novel approach for the design of end-user debugging tools: a focus on supporting end users' effective debugging strategies. This paper makes two contributions. We first demonstrate the potential of a strategy-centric approach to tool design by presenting StratCel, an add-in for Excel. Second, we show the benefits of this design approach: participants using StratCel found twice as many bugs as participants using standard Excel, they fixed four times as many bugs, and all this in only a small fraction of the time. Other contributions included: a boost in novices' debugging performance near experienced participants' improved levels, validated design guidelines, a discussion of the generalizability of this approach, and several opportunities for future research. {\textcopyright} 2010 ACM.},
author = {Grigoreanu, Valentina I. and Burnett, Margaret M. and Robertson, George G.},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/1753326.1753431},
isbn = {9781605589299},
keywords = {debugging strategies,debugging tools,end-user software engineering,tool design},
pages = {713--722},
title = {{A strategy-centric approach to the design of end-user debugging tools}},
volume = {2},
year = {2010}
}
@article{Xiaobo2021,
abstract = {Automatic fault localization is essential for software engineering. However, fault localization suffers from the interactions among multiple faults. Our previous research revealed that the fault-coupling effect is responsible for the weakened fault localization performance in multiple-fault programs. On the basis of this finding, we propose a Test Case Restoration Method based on the Genetic Algorithm (TRGA) to search potential coupling test cases and conduct a restoration process for eliminating the coupling effect. The major contributions of the current study are as follows: (1) the construction of a fitness function to measure the possibility of failed test cases becoming coupling test cases; (2) the development of a TRGA that searches potential coupling test cases; (3) and an evaluation of the TRGA efficiency across 14 open-source programs, three spectrum-based fault localizations, and two parallel debugging techniques. The results revealed the TRGA outperformed the original fault localization techniques in 74.28% and 78.57% of the scenarios in the best and worst cases, respectively. On average, the percentage improvement was 4.43% for the best case and 2% for the worst case. A detailed discussion of TRGA parameter settings is also provided.},
author = {Xiaobo, Yan and Bin, Liu and Shihai, Wang},
doi = {10.1016/j.jss.2020.110861},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Evolution algorithm,Fault localization,Multiple-faults,Software debugging,Test restoration},
month = {feb},
pages = {110861},
publisher = {Elsevier Inc.},
title = {{A Test Restoration Method based on Genetic Algorithm for effective fault localization in multiple-fault programs}},
volume = {172},
year = {2021}
}
@incollection{Johnson2017,
abstract = {CAD is a critical tool for engineers in the 21st century. To improve CAD usage and education, methods for assessing and evaluating modeling procedures and decision making are necessary. To this end, two common verbal data collection methods are assessed for analyzing CAD modeling procedures. Stimulated recall and concurrent think aloud are compared to each other and screen capture video data. While the concurrent think aloud method seems to increase the necessary modeling time, the think aloud requirement does not affect the proportion of time spent on particular activities. A novel method of using Cohens Kappa with time usage data was implemented to compare the audio methods to screen capture video data. Neither audio method showed significant agreement with the video data when corrected for chance agreement. It is likely that both video and audio data are required to observe significant insights with respect to CAD modeling procedures and decisions. Drawbacks and benefits associated with alternative methods are also highlighted.},
author = {Johnson, Michael D. and Ye, Karl},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-58071-5_24},
issn = {16113349},
keywords = {Design: analysis and design methods,Stimulated recall,Think aloud,UX and usability: evaluation methods and technique},
pages = {313--324},
publisher = {Springer Verlag},
title = {{An analysis of CAD modeling procedure data collection using synchronous and retrospective think aloud techniques}},
volume = {10271},
year = {2017}
}
@article{Zhang2019,
abstract = {Manual debugging is notoriously tedious and time-consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. The focus of the existing SBFL techniques is to consider how to differentiate program entities (i.e., one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance the existing SBFL techniques by additionally considering how to differentiate tests (i.e., the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work. We present our basic approach, PRFL, a lightweight technique that boosts SBFL by differentiating tests using PageRank algorithm. Specifically, given the original program spectrum information, PRFL uses PageRank to recompute the spectrum by considering the contributions of different tests. Next, traditional SBFL techniques are applied on the recomputed spectrum to achieve more effective fault localization. On top of PRFL, we explore PRFL+ and PRFL MA , two variants which extend PRFL by optimizing its components and integrating Method-level Aggregation technique, respectively. Though being simple and lightweight, PRFL has been demonstrated to outperform state-of-the-art SBFL techniques significantly (e.g., ranking 39.2% / 82.3% more real/artificial faults within Top-1 compared with the most effective traditional SBFL technique) with low overhead (e.g., around 6 minutes average extra overhead on real faults) on 395 real faults from 6 Defects4J projects and 96925 artificial (i.e., mutation) faults from 240 GitHub projects. To further validate PRFL's effectiveness, we compare PRFL with multiple recent proposed fault localization techniques (e.g., Multric, Metallaxis and MBFL-hybrid-avg), and the experimental results show that PRFL outperforms them as well. Furthermore, we study the performance of PRFL MA , and the experimental results present it can locate 137 real faults (73.4% / 24.5% more compared with the most effective SBFL/PRFL technique) and 35058 artificial faults (159.6% / 28.1% more than SBFL/PRFL technique) at Top-1. At last, we study the generalizability of PRFL on another benchmark Bugs.jar, and the result shows PRFL can help locate around 30% more faults at Top 1.},
author = {Zhang, Mengshi and Li, Yaoxian and Li, Xia and Chen, Lingchao and Zhang, Yuqun and Zhang, Lingming and Khurshid, Sarfraz},
doi = {10.1109/tse.2019.2911283},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {apr},
number = {6},
pages = {1089--1113},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{An Empirical Study of Boosting Spectrum-Based Fault Localization via PageRank}},
volume = {47},
year = {2019}
}
@article{Zou2021,
abstract = {The performance of fault localization techniques is critical to their adoption in practice. This paper reports on an empirical study of a wide range of fault localization techniques on real-world faults. Different from previous studies, this paper (1) considers a wide range of techniques from different families, (2) combines different techniques, and (3) considers the execution time of different techniques. Our results reveal that a combined technique significantly outperforms any individual technique (200 percent increase in faults localized in Top 1), suggesting that combination may be a desirable way to apply fault localization techniques and that future techniques should also be evaluated in the combined setting. Our implementation is publicly available for evaluating and combining fault localization techniques.},
archivePrefix = {arXiv},
arxivId = {1803.09939},
author = {Zou, Daming and Liang, Jingjing and Xiong, Yingfei and Ernst, Michael D. and Zhang, Lu},
doi = {10.1109/TSE.2019.2892102},
eprint = {1803.09939},
issn = {19393520},
journal = {IEEE Transactions on Software Engineering},
keywords = {Fault localization,empirical study,learning to rank,program debugging,software testing},
month = {feb},
number = {2},
pages = {332--347},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{An Empirical Study of Fault Localization Families and Their Combinations}},
volume = {47},
year = {2021}
}
@inproceedings{Insa2011,
abstract = {Algorithmic debugging is a technique that uses an internal data structure to represent computations and ask about their correctness. The strategy used to explore this data structure is essential for the performance of the technique. The most efficient strategy in practice is Divide and Query that, until now, has been considered optimal in the worst case. In this paper we first show that the original algorithm is inaccurate and moreover, in some situations it is unable to find all possible solutions, thus it is incomplete. Then, we present a new version of the algorithm that solves these problems. Moreover, we introduce a counterexample showing that Divide and Query is not optimal, and we propose the first optimal strategy for algorithmic debugging with respect to the number of questions asked by the debugger. {\textcopyright} 2011 IEEE.},
author = {Insa, David and Silva, Josep},
booktitle = {2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings},
doi = {10.1109/ASE.2011.6100055},
isbn = {9781457716393},
keywords = {Algorithmic Debugging,Divide and Query},
pages = {203--212},
title = {{An optimal strategy for algorithmic debugging}},
year = {2011}
}
@article{Zhang2017,
abstract = {Ontologies in real-world applications are typically dynamic entities that are frequently modified when new knowledge needs to be added or when existing knowledge is no longer considered valid. Logical errors inevitably occur when ontologies are modified. To effectively identify the problematic axioms that are responsible for these logical errors, an optimization strategy based on the clash sequence strategy is proposed for debugging the incoherent terminologies in dynamic environments. The clash sequence strategy is used to identify the clash set from an incoherent terminology, and then the debugging work can be performed on the identified clash set than on the entire terminology. A heuristic strategy is also proposed to reuse the results of the previous debugging and to provide information for the next debugging. The experiment results show that the proposed debugging approach based on clash sequences can achieve a significant improvement especially for large-scale ontologies in many cases.},
author = {Zhang, Yu and Ouyang, Dantong and Ye, Yuxin},
doi = {10.1109/ACCESS.2017.2758521},
issn = {21693536},
journal = {IEEE Access},
keywords = {Ontology debugging,clash sequences,clash set,description logics,incoherent terminologies},
month = {oct},
pages = {24284--24300},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{An Optimization Strategy for Debugging Incoherent Terminologies in Dynamic Environments}},
volume = {5},
year = {2017}
}
@inproceedings{Whalley2021,
abstract = {Debugging code is a complex task that requires knowledge about the mechanics of a programming language, the purpose of a given program, and an understanding of how the program achieves the purpose intended. It is generally accepted that prior experience with similar bugs improves the debugging process and that a systematic process is needed to be able to successfully move from the symptoms of a bug to the cause. Students who are learning to program may struggle with one or more aspect of debugging, and anecdotally, spend a lot of their time debugging faulty code. In this paper we analyse student answers to questions designed to focus student attention on the symptoms of a bug and to use those symptoms to generate a hypothesis about the cause of a bug. To ensure students focus on the symptoms rather than the code, we use paper-based exercises that ask students to reflect on various bugs and to hypothesize about the cause. We analyse the students' responses to the questions and find that using our structured process most students are able to generalize from a single failing test case to the likely problem in the code, but they are much less able to identify the appropriate location or an actual fix.},
author = {Whalley, Jacqueline and Settle, Amber and Luxton-Reilly, Andrew},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3441636.3442300},
isbn = {9781450376860},
keywords = {debugging,debugging process,teaching},
month = {feb},
pages = {11--20},
publisher = {Association for Computing Machinery},
title = {{Analysis of a Process for Introductory Debugging}},
year = {2021}
}
@article{Ni2020,
abstract = {During the bug fixing process, developers usually need to analyze the source code to induce the bug cause, which is useful for bug understanding and localization. The bug fixes of historical bugs usually reflects the bug causes when fixing them. This paper aims at exploiting the corresponding relationship between bug causes and bug fixes to automatically classify bugs into their cause categories. First, we define the code-related bug classification criterion from the perspective of the cause of bugs. Then, we propose a new model to exploit the knowledge in the bug fix by constructing fix trees from the diff source code at Abstract Syntax Tree (AST) level, and representing each fix tree based on the encoding method of Tree-based Convolutional Neural Network (TBCNN). Finally, the corresponding relationship between bug causes and bug fixes is analyzed by automatically classifying bugs into their cause categories. We collected 2000 real-world bugs from two open source projects Mozilla and Radare2 to evaluate our approach. The experimental results show the existence of observational correlation between the bug fix and the cause of the historical bugs, and the proposed fix tree can effectively express the characteristics of the historical bugs for bug cause classification.},
author = {Ni, Zhen and Li, Bin and Sun, Xiaobing and Chen, Tianhao and Tang, Ben and Shi, Xinchen},
doi = {10.1016/j.jss.2020.110538},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Bug analysis,Bug cause classification,Fix tree,TBCNN},
month = {may},
pages = {110538},
publisher = {Elsevier Inc.},
title = {{Analyzing bug fix for automatic bug cause classification}},
volume = {163},
year = {2020}
}
@article{Chen2018,
abstract = {The concurrent think-aloud protocol (CTA) is an effective method for collecting abundant product comments related to user satisfaction during the execution of evaluation tasks. However, manual analysis of these audio comments is time-consuming and labor-intensive. This paper aims to propose an approach for automated comprehensive evaluation of user interface (UI) satisfaction. It takes advantage of text mining and sentiment analysis (SA) techniques instead of manual analysis in order to assess user comments collected by the CTA. Based on the results of the SA, the proposed approach makes use of the analytic hierarchy process (AHP) method to evaluate the overall satisfaction and support developers for UI design improvements. In order to enhance the objectivity of evaluation, a sentiment matrix originating from text mining and SA on user comments is used to replace the criteria and the relative weights of the AHP method which were previously defined by experts. A comparison between the questionnaire survey method and the proposed approach in the empirical study suggested that the latter can efficiently evaluate UI satisfaction with high accuracy and provide designers abundant and specific information directly related to defects in design. It is argued that the proposed approach could be used as an automated framework for handling any type of comments.},
author = {Chen, Weipeng and Lin, Tao and Chen, Li and Yuan, Peisa},
doi = {10.1007/s10209-018-0610-z},
issn = {16155297},
journal = {Universal Access in the Information Society},
keywords = {Analytic hierarchy process,Concurrent think-aloud,Satisfaction evaluation,Sentiment analysis,Text mining},
month = {aug},
number = {3},
pages = {635--647},
publisher = {Springer Verlag},
title = {{Automated comprehensive evaluation approach for user interface satisfaction based on concurrent think-aloud method}},
volume = {17},
year = {2018}
}
@article{Fan2020,
abstract = {Think-aloud protocols are a highly valued usability testing method for identifying usability problems. Despite the value of conducting think-aloud usability test sessions, analyzing think-aloud sessions is often time-consuming and labor-intensive. Consequently, previous research has urged the community to develop techniques to support fast-paced analysis. In this work, we took the first step to design and evaluate machine learning (ML) models to automatically detect usability problem encounters based on users' verbalization and speech features in think-aloud sessions. Inspired by recent research that shows subtle patterns in users' verbalizations and speech features tend to occur when they encounter problems, we examined whether these patterns can be utilized to improve the automatic detection of usability problems. We first conducted and recorded think-aloud sessions and then examined the effect of different input features, ML models, test products, and users on usability problem encounters detection. Our work uncovers several technical and user interface design challenges and sets a baseline for automating usability problem detection and integrating such automation into UX practitioners' workflow.},
author = {Fan, Mingming and Li, Yue and Truong, Khai N.},
doi = {10.1145/3385732},
issn = {21606463},
journal = {ACM Transactions on Interactive Intelligent Systems},
keywords = {AI-assisted UX analysis method,Think aloud,machine learning,speech features,usability problem,user experience (UX),verbalization},
month = {jun},
number = {2},
pages = {16:1--16:24},
publisher = {Association for Computing Machinery},
title = {{Automatic Detection of Usability Problem Encounters in Think-aloud Sessions}},
volume = {10},
year = {2020}
}
@article{Hinostroza2018,
abstract = {Many studies show that a large percentage of students lack the digital competencies to solve information problems using the internet. To address this problem, researchers have developed models to structure the process. However, we maintain that it is first necessary to understand how students actually search for information online when solving an information problem and compare their behaviours with the available literature. To do this, we collected and analysed the data on a laboratory setting of 41 students while they solved 16 information problems using the internet and expressed aloud what they were thinking whilst solving them. We identified 21 categories of actions, grouped into 9 activity types associated with four information search processes, showing that some of the students' stereotyped search behaviours are in fact conscious decisions based on the requirements of the task and the answers found. The results also show that searching for information on the internet can not only vary depending on the task's requirements, but also on the number of iterations students need to perform to reach an answer. These findings provide some evidence that challenge the assumption that students simply lack the competencies to search for information on the internet; rather, it seems that they show an awareness of different strategies, which they decide to use based on the context and purpose of the task, making their search behaviour more elaborated and complex than is usually portrayed by researchers in the field. Based on this, implications and further research lines are also discussed.},
author = {Hinostroza, J. Enrique and Ibieta, Andrea and Labb{\'{e}}, Christian and Soto, Mar{\'{i}}a Teresa},
doi = {10.1007/s10639-018-9698-2},
issn = {15737608},
journal = {Education and Information Technologies},
keywords = {Computer literacy,Digital competencies,Information problem-solving,Search strategies,Student internet search},
month = {sep},
number = {5},
pages = {1933--1953},
publisher = {Springer New York LLC},
title = {{Browsing the internet to solve information problems: A study of students' search actions and behaviours using a ‘think aloud' protocol}},
volume = {23},
year = {2018}
}
@inproceedings{Meier2019,
abstract = {We describe our efforts to compare data collection methods using two think-aloud protocols in preparation to be used as a basis for automatic structuring and labeling of a large database of high-dimensional human activities data into a valuable resource for research in cognitive robotics. The envisioned dataset, currently in development, will contain synchronously recorded multimodal data, including audio, video, and biosignals (eye-tracking, motion-tracking, muscle and brain activity) from about 100 participants performing everyday activities while describing their task through use of think-aloud protocols. This paper provides details of our pilot recordings in the well-established and scalable “table setting scenario,” describes the concurrent and retrospective think-aloud protocols used, the methods used to analyze them, and compares their potential impact on the data collected as well as the automatic data segmentation and structuring process.},
author = {Meier, Moritz and Mason, Celeste and Putze, Felix and Schultz, Tanja},
booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
doi = {10.21437/Interspeech.2019-3072},
issn = {19909772},
keywords = {Activities of daily living,Biosignals,Cognitive robotics,Multimodal,Think-aloud},
pages = {559--563},
publisher = {International Speech Communication Association},
title = {{Comparative analysis of think-aloud methods for everyday activities in the context of cognitive robotics}},
volume = {2019-Septe},
year = {2019}
}
@article{Alhadreti2021,
abstract = {This paper presents the results of a study that aimed to compare the utility and validity of the traditional concurrent think-aloud and the co-discovery usability testing methods. The study was conducted in Saudi Arabia and involved three points of comparison: number and nature of usability problems discovered, test participants' experiences, and overall task performance. The results show significant differences between the two types of testing methods. The co-discovery method led to the detection of a greater number of minor usability problems relating to layout and functionality. The participants also found the co-discovery method to be easier and less tiring to perform and more natural for them than the concurrent think-aloud method. No difference was found between the methods in terms of participants' task performance. The study concludes that the co-discovery method seems to be appropriate for identifying numerous minor issues and ensuring that the usability testing experience is as natural as possible for participants. However, the classic method seems to be a more cost-effective method, as it is equally useful in revealing high-severity problems and requires only one participant per test session.},
author = {Alhadreti, Obead},
doi = {10.1080/10447318.2020.1809152},
issn = {15327590},
journal = {International Journal of Human-Computer Interaction},
number = {2},
pages = {118--130},
publisher = {Bellwether Publishing, Ltd.},
title = {{Comparing Two Methods of Usability Testing in Saudi Arabia: Concurrent Think-Aloud vs. Co-Discovery}},
volume = {37},
year = {2021}
}
@article{Fan2019,
abstract = {The concurrent think-aloud protocol-inwhich participants verbalize their thoughts when performing tasks- is a widely employed approach in usability testing. Despite its value, analyzing think-aloud sessions can be onerous because it often entails assessing all of a user's verbalizations. This has motivated previous research on developing categories to segment verbalizations into manageable units of analysis. However, the way in which a category might relate to usability problems is currently unclear. In this research, we sought to address this gap in our understanding. We also studied how speech features might relate to usability problems. Through two studies, this research demonstrates that certain patterns of verbalizations are more telling of usability problems than others and that these patterns are robust to different types of test products (i.e., physical devices and digital systems), access to different types of information (i.e., video and audio modality), and the presence or absence of a visualization of verbalizations. The implication is that the verbalization and speech patterns can potentially reduce the time and effort required for analysis by enabling evaluators to focus more on the important aspects of a user's verbalizations. The patterns could also potentially be used to inform the design of systems to automatically detect when in the recorded think-aloud sessions users experience problems.},
author = {Fan, Mingming and Lin, Jinglan and Chung, Christina and Truong, Khai N.},
doi = {10.1145/3325281},
issn = {15577325},
journal = {ACM Transactions on Computer-Human Interaction},
keywords = {Concurrent think-aloud,Loudness,Pitch,Sentiment,Silence,Speech features,Speech rate,Usability problems,Usability testing,Verbal fillers,Verbalization,Verbalization categories},
month = {jul},
number = {5},
pages = {28:1--28:35},
publisher = {Association for Computing Machinery},
title = {{Concurrent think-aloud verbalizations and usability problems}},
volume = {26},
year = {2019}
}
@article{Almhana2021,
abstract = {Software development teams need to deal with several open reports of critical bugs to be addressed urgently and simultaneously. The management of these bugs is a complex problem due to the limited resources and the deadlines-pressure. Most of the existing studies treated bug reports in isolation when assigning them to developers. Thus, developers may spend considerable cognitive efforts moving between completely unrelated bug reports thus not sharing any common files to be inspected. In this paper, we propose an automated bugs triage approach based on the dependencies between the open bug reports. Our approach starts by localizing the files to be inspected for each of the pending bug reports. We defined the dependency between two bug reports as the number of common files to be inspected to localize the bugs. Then, we adopted multi-objective search to rank the bug reports for programmers based on both their priorities and the dependency between them. We evaluated our approach on a set of open source programs and compared it to the traditional approach of considering bug reports in isolation based mainly on their priority. The results show a significant time reduction of over 30% in localizing the bugs simultaneously comparing to the traditional bugs prioritization technique based on only priorities.},
author = {Almhana, Rafi and Kessentini, Marouane},
doi = {10.1007/s10515-020-00279-2},
file = {::},
issn = {15737535},
journal = {Automated Software Engineering},
keywords = {Bug localization,Bug prioritization,Bug triage,Bugs management,Search-based software engineering,Software quality assurance},
month = {may},
number = {1},
pages = {1},
publisher = {Springer},
title = {{Considering dependencies between bug reports to improve bugs triage}},
volume = {28},
year = {2021}
}
@inproceedings{Johanssen2019,
abstract = {Thinking Aloud is a method that allows the collection of expressive user feedback for software improvement. However, its frequent application in a rapid development processes such as Continuous Software Engineering (CSE) is challenging, since repetitively performing manual observations and evaluations demand high effort. We propose the Continuous Thinking Aloud (CTA) approach for conducting Thinking Aloud during CSE. CTA records speech feedback for a user who starts using a new feature increment. The recordings are automatically transcribed and classified into one of four feedback categories that differentiate between insecure, neutral, positive, and negative sentiments. CTA visualizes these feedback classifications on a sentence level, next to its related high-level change of the feature increment. This supports developers in problem discovery, in particular regarding usability. CTA integrates with CSE processes and represents a scalable approach enabling repeated application during the software development lifecycle.},
author = {Johanssen, Jan Ole and Reimer, Lara Marie and Bruegge, Bernd},
booktitle = {Proceedings - 2019 IEEE/ACM Joint 4th International Workshop on Rapid Continuous Software Engineering and 1st International Workshop on Data-Driven Decisions, Experimentation and Evolution, RCoSE/DDrEE 2019},
doi = {10.1109/RCoSE/DDrEE.2019.00010},
isbn = {9781728122472},
keywords = {Classifier,Continuous software engineering,Feature crumb,Thinking aloud,Usability engineering,Visualization},
month = {may},
pages = {12--15},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Continuous thinking aloud}},
year = {2019}
}
@article{Tan2021,
abstract = {Many automated program repair techniques have been proposed for fixing bugs. Some of these techniques use the information beyond the given buggy program and test suite to improve the quality of generated patches. However, there are several limitations that hinder the wide adoption of these techniques, including (1) they rely on a fixed set of repair templates for patch generation or reference implementation, (2) searching for the suitable reference implementation is challenging, (3) generated patches are not explainable. Meanwhile, a recent approach shows that similar bugs exist across different projects and one could use the GitHub issue from a different project for finding new bugs for a related project. We propose collaborative bug fixing, a novelapproach that suggests bug reports that describe a similar bug. Our studyredefines similar bugs as bugs that share the (1) same libraries, (2) same functionalities, (3) same reproduction steps, (4) same configurations, (5) sameoutcomes, or (6) same errors. Moreover, our study revealed the usefulness of similar bugs in helping developers in finding more context about the bug and fixing. Based on our study, we design CrossFix, a tool that automatically suggests relevant GitHub issues based on an open GitHub issue. Our evaluation on 249 open issues from Java and Android projects shows that CrossFix could suggest similar bugs to help developers in debugging and fixing.},
archivePrefix = {arXiv},
arxivId = {2103.13453},
author = {Tan, Shin Hwei and Li, Ziqiang and Yan, Lu},
eprint = {2103.13453},
journal = {CoRR},
month = {mar},
title = {{CrossFix: Collaborative bug fixing by recommending similar bugs}},
url = {https://arxiv.org/abs/2103.13453 http://arxiv.org/abs/2103.13453},
volume = {abs/2103.1},
year = {2021}
}
@article{Hopkins2006,
abstract = {On-chip program and data tracing is now an essential part of any system level development platform for System-on-Chip (SoC). Current debug support solutions are platform specific and incompatible with processors and active peripherals from other sources, restricting effective design reuse. In order to overcome this reuse challenge, this paper defines interfaces to decouple the debug support from processor cores and other active data accessing units. The on-chip debug support infrastructure is also decoupled from each core's debug support and from the trace port or trace memory, using an additional interface. As a result, this decoupling of the debug support infrastructure provides freedom from a specific SoC platform. These interfaces are applied through a reference 1 design modeled using VHDL that is based on a novel low overhead trace message framework. Compared with a leading implementation of a relevant standard, the reference design is 50 percent more compact while providing improvements in trace compression of 8.4 percent for program trace messages and almost 24 percent for data trace messages. This reference design is a multiple core solution that is compatible with most SoC architectures, including those based on emerging Network-on-Chip architectures. {\textcopyright} 2006 IEEE.},
author = {Hopkins, Andrew B.T. and McDonald-Maier, Klaus D.},
doi = {10.1109/TC.2006.22},
issn = {00189340},
journal = {IEEE Transactions on Computers},
keywords = {Debugging aids,Multiprocessor systems,Real-time and embedded systems,System architectures, integration, and modeling},
month = {feb},
number = {2},
pages = {174--184},
title = {{Debug support strategy for systems-on-chips with multiple processor cores}},
volume = {55},
year = {2006}
}
@article{Fields2021,
abstract = {Much attention in constructionism has focused on designing tools and activities that support learners in designing fully finished and functional applications and artefacts to be shared with others. But helping students learn to debug their applications often takes on a surprisingly more instructionist stance by giving them checklists, teaching them strategies or providing them with test programmes. The idea of designing bugs for learning—or debugging by design—makes learners agents of their own learning and, more importantly, of making and solving mistakes. In this paper, we report on our implementation of ‘Debugging by Design' activities in a high school classroom over a period of 8 hours as part of an electronic textiles unit. Students were tasked to craft the electronic textile artefacts with problems or bugs for their peers to solve. Drawing on observations and interviews, we answer the following research questions: (1) How did students participate in making bugs for others? (2) What did students gain from designing and solving bugs for others? In the discussion, we address the opportunities and challenges that designing personally and socially meaningful failure artefacts provides for becoming objects-to-think-with and objects-to-share-with in student learning and promoting new directions in constructionism. Practitioner notes What is already known about this topic There is substantial evidence for the benefits of learning programming and debugging in the context of constructing personally relevant and complex artefacts, including electronic textiles. Related, work on productive failure has demonstrated that providing learners with strategically difficult problems (in which they ‘fail') equips them to better handle subsequent challenges. What this paper adds In this paper, we argue that designing bugs or ‘failure artefacts' is as much a constructionist approach to learning as is designing fully functional artefacts. We consider how ‘failure artefacts' can be both objects-to-learn-with and objects-to-share-with. We introduce the concept of ‘Debugging by Design' (DbD) as a means to expand application of constructionism to the context of developing ‘failure artifacts'. Implications for practice and/or policy We conceptualise a new way to enable and empower students in debugging—by designing creative, multimodal buggy projects for others to solve. The DbD approach may support students in near-transfer of debugging and the beginning of a more systematic approach to debugging in later projects and should be explored in other domains beyond e-textiles. New studies should explore learning, design and teaching that empower students to design bugs in projects in mischievous and creative ways.},
author = {Fields, Deborah A. and Kafai, Yasmin B. and Morales-Navarro, Luis and Walker, Justice T.},
doi = {10.1111/bjet.13079},
issn = {14678535},
journal = {British Journal of Educational Technology},
keywords = {computer science education,debugging,e-textiles,physical computing,productive failure},
month = {may},
number = {3},
pages = {1078--1092},
publisher = {Blackwell Publishing Ltd},
title = {{Debugging by design: A constructionist approach to high school students' crafting and coding of electronic textiles as failure artefacts}},
volume = {52},
year = {2021}
}
@article{Zhao2020,
abstract = {Logic programming languages such as Datalog have become popular as Domain Specific Languages (DSLs) for solving large-scale, real-world problems, in particular, static program analysis and network analysis. The logic specifications that model analysis problems process millions of tuples of data and contain hundreds of highly recursive rules. As a result, they are notoriously difficult to debug. While the database community has proposed several data provenance techniques that address the Declarative Debugging Challenge for Databases, in the cases of analysis problems, these state-of-the-art techniques do not scale. In this article, we introduce a novel bottom-up Datalog evaluation strategy for debugging: Our provenance evaluation strategy relies on a new provenance lattice that includes proof annotations and a new fixed-point semantics for semi-na{\"{i}}ve evaluation. A debugging query mechanism allows arbitrary provenance queries, constructing partial proof trees of tuples with minimal height. We integrate our technique into Souffl{\'{e}}, a Datalog engine that synthesizes C++ code, and achieve high performance by using specialized parallel data structures. Experiments are conducted with DOOP/DaCapo, producing proof annotations for tens of millions of output tuples. We show that our method has a runtime overhead of 1.31× on average while being more flexible than existing state-of-the-art techniques.},
author = {Zhao, David and Suboti{\'{c}}, Pavle and Scholz, Bernhard},
doi = {10.1145/3379446},
issn = {15584593},
journal = {ACM Transactions on Programming Languages and Systems},
keywords = {Static analysis,datalog,provenance},
month = {may},
number = {2},
pages = {7:1--7:35},
publisher = {Association for Computing Machinery},
title = {{Debugging Large-scale Datalog: A Scalable Provenance Evaluation Strategy}},
volume = {42},
year = {2020}
}
@article{Drain2021,
abstract = {The joint task of bug localization and program repair is an integral part of the software development process. In this work we present DeepDebug, an approach to automated debugging using large, pretrained transformers. We begin by training a bug-creation model on reversed commit data for the purpose of generating synthetic bugs. We apply these synthetic bugs toward two ends. First, we directly train a backtranslation model on all functions from 200K repositories. Next, we focus on 10K repositories for which we can execute tests, and create buggy versions of all functions in those repositories that are covered by passing tests. This provides us with rich debugging information such as stack traces and print statements, which we use to finetune our model which was pretrained on raw source code. Finally, we strengthen all our models by expanding the context window beyond the buggy function itself, and adding a skeleton consisting of that function's parent class, imports, signatures, docstrings, and method bodies, in order of priority. On the QuixBugs benchmark, we increase the total number of fixes found by over 50%, while also decreasing the false positive rate from 35% to 5% and decreasing the timeout from six hours to one minute. On our own benchmark of executable tests, our model fixes 68% of all bugs on its first attempt without using traces, and after adding traces it fixes 75% on first attempt. We will open-source our framework and validation set for evaluating on executable tests.},
archivePrefix = {arXiv},
arxivId = {2105.09352},
author = {Drain, Dawn and Clement, Colin B. and Serrato, Guillermo and Sundaresan, Neel},
eprint = {2105.09352},
journal = {CoRR},
title = {{DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code Skeletons}},
url = {https://arxiv.org/abs/2105.09352 http://arxiv.org/abs/2105.09352},
volume = {abs/2105.0},
year = {2021}
}
@inproceedings{Wardat2021,
abstract = {Deep neural networks (DNNs) are becoming an integral part of most software systems. Previous work has shown that DNNs have bugs. Unfortunately, existing debugging techniques do not support localizing DNN bugs because of the lack of understanding of model behaviors. The entire DNN model appears as a black box. To address these problems, we propose an approach that automatically determines whether the model is buggy or not, and identifies the root causes. Our key insight is that historic trends in values propagated between layers can be analyzed to identify faults, and localize faults. To that end, we first enable dynamic analysis of deep learning applications: by converting it into an imperative representation and alternatively using a callback mechanism. Both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the DNN while it is being trained on the training data. We then conduct dynamic analysis over the traces to identify the faulty layer that causes the error. We propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer on the DNN outcome. We have collected a benchmark containing 40 buggy models and patches that contain real errors in deep learning applications from Stack Overflow and GitHub. Our benchmark can be used to evaluate automated debugging tools and repair techniques. We have evaluated our approach using this DNN bug-and-patch benchmark, and the results showed that our approach is much more effective than the existing debugging approach used in the state of the practice Keras library. For 34 out of 40 cases, our approach was able to detect faults whereas the best debugging approach provided by Keras detected 32 out of 40 faults. Our approach was able to localize 21 out of 40 bugs whereas Keras did not localize any faults.},
archivePrefix = {arXiv},
arxivId = {2103.03376},
author = {Wardat, Mohammad and Le, Wei and Rajan, Hridesh},
booktitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
doi = {10.1109/icse43902.2021.00034},
eprint = {2103.03376},
file = {:C\:/Users/geryxyz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wardat, Le, Rajan - 2021 - DeepLocalize Fault Localization for Deep Neural Networks.pdf:pdf},
keywords = {Debug-ging,Deep learning bugs,Fault Location,Index Terms-Deep Neural Networks,Program Analysis},
pages = {251--262},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{DeepLocalize: Fault Localization for Deep Neural Networks}},
url = {http://arxiv.org/abs/2103.03376},
year = {2021}
}
@article{Chen2021,
abstract = {Logs in bug reports provide important debugging information for developers. During the debugging process, developers need to study the bug report and examine user-provided logs to understand the system executions that lead to the problem. Intuitively, user-provided logs illustrate the problems that users encounter and may help developers with the debugging process. However, some logs may be incomplete or inaccurate, which can cause difficulty for developers to diagnose the bug, and thus, delay the bug fixing process. In this paper, we conduct an empirical study on the challenges that developers may encounter when analyzing the user-provided logs and their benefits. In particular, we study both log snippets and exception stack traces in bug reports. We conduct our study on 10 large-scale open-source systems with a total of 1,561 bug reports with logs (BRWL) and 7,287 bug reports without logs (BRNL). Our findings show that: 1) BRWL takes longer time (median ranges from 3 to 91 days) to resolve compared to BRNL (median ranges from 1 to 25 days). We also find that reporters may not attach accurate or sufficient logs (i.e., developers often ask for additional logs in the Comments section of a bug report), which extends the bug resolution time. 2) Logs often provide a good indication of where a bug is located. Most bug reports (73%) have overlaps between the classes that generate the logs and their corresponding fixed classes. However, there is still a large number of bug reports where there is no overlap between the logged and fixed classes. 3) Our manual study finds that there is often missing system execution information in the logs. Many logs only show the point of failure (e.g., exception) and do not provide a direct hint on the actual root cause. In fact, through call graph analysis, we find that 28% of the studied bug reports have the fixed classes reachable from the logged classes, while they are not visible in the logs attached in bug reports. In addition, some logging statements are removed in the source code as the system evolves, which may cause further challenges in analyzing the logs. In short, our findings highlight possible future research directions to better help practitioners attach or analyze logs in bug reports.},
author = {Chen, An Ran and Chen, Tse Hsun and Wang, Shaowei},
doi = {10.1007/s10664-020-09893-w},
file = {::},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Bug report,Empirical study,Log,Stack trace},
month = {jan},
number = {1},
pages = {8},
publisher = {Springer},
title = {{Demystifying the challenges and benefits of analyzing user-reported logs in bug reports}},
volume = {26},
year = {2021}
}
@inproceedings{Salminen2019,
abstract = {Increased access to data and computational techniques enable innovations in the space of automated customer analytics, for example, automatic persona generation. Automatic persona generation is the process of creating data-driven representations from user or customer statistics. Even though automatic persona generation is technically possible and provides advantages compared to manual persona creation regarding the speed and freshness of the personas, it is not clear (a) what information to include in the persona profiles and (b) how to display that information. To query into these aspects relating information design of personas, we conducted a user study with 38 participants. In the findings, we report several challenges relating to the design of automatically generated persona profiles, including usability issues, perceptual issues, and issues relating to information content. Our research has implications for the information design of data-driven personas.},
author = {Salminen, Joni and Şeng{\"{u}}n, Sercan and Jung, Soon Gyo and Jansen, Bernard J.},
booktitle = {CHIIR 2019 - Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
doi = {10.1145/3295750.3298942},
isbn = {9781450360258},
keywords = {Automatically generated personas,Data-driven personas,Information design,Personas,User study},
month = {mar},
pages = {225--229},
publisher = {Association for Computing Machinery, Inc},
title = {{Design issues in automatically generated persona profiles: A qualitative analysis from 38 think-aloud transcripts}},
year = {2019}
}
@inproceedings{Yamashita2018,
abstract = {There are two well-known difficulties to test and interpret methodologies for mining developer interaction traces: first, the lack of enough large datasets needed by mining or machine learning approaches to provide reliable results; and second, the lack of "ground truth" or empirical evidence that can be used to triangulate the results, or to verify their accuracy and correctness. Moreover, relying solely on interaction traces limits our ability to take into account contextual factors that can affect the applicability of mining techniques in other contexts, as well hinders our ability to fully understand the mechanics behind observed phenomena. The data presented in this paper attempts to alleviate these challenges by providing 600+ hours of developer interaction traces, from which 26+ hours are backed with video recordings of the IDE screen and developer's comments. This data set is relevant to researchers interested in investigating program comprehension, and those who are developing techniques for interaction traces analysis and mining.},
author = {Yamashita, Aiko and Petrillo, Fabio and Khomh, Foutse and Gu{\'{e}}h{\'{e}}neuc, Yann Ga{\"{e}}l},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/3196398.3196457},
isbn = {9781450357166},
issn = {02705257},
keywords = {empirical study,industrial data,interaction traces,log mining,program comprehension,programming flow},
month = {may},
pages = {50--53},
publisher = {IEEE Computer Society},
title = {{Developer interaction traces backed by IDE screen recordings from think aloud sessions}},
year = {2018}
}
@article{Xiaobo2021a,
abstract = {Context: Automatic fault localization is essential to intelligent software system. Most fault localization techniques assume the test oracle is perfect before debugging, which is hard to exist in practice. In fact, the test suite would contain a number of unlabelled test cases which have been proved to be useful in fault localization. However, due to the execution diversity, not all unlabelled test cases are suitable for fault localization. Selecting inappropriate unlabelled test cases can even weaken the fault localization efficiency. Objective: To solve the problem of filtering unlabelled test cases, this work aims to construct a feasible framework to select suitable unlabelled test cases for better fault localization. Method: To address this issue, an entropy-based framework Efilter is constructed to filter unlabelled test cases. In Efilter, a Statement-based entropy and Testsuite-based entropy are constructed to measure the localization uncertainty of given test suite. The unlabelled test case with less Statement-based entropy or Testsuite-based entropy compared with its threshold would be selected. Further, the feature integration strategies for both Statement-based entropy and Testsuite-based entropy are given to calculate the suspiciousness of statements. Results: The Efilter efficiency is evaluated across 6 open-source programs and 3 spectrum-based fault localizations. The results reveal that Efilter can improve fault localization efficiency by 18.8% and 16.5% with the Statement-based entropy and the Testsuite-based entropy respectively compared with the strategy without Efilter from the perspective of EXAM score on average. Conclusion: Our results indicate that the Efilter with both the Statement-based entropy and the Testsuite-based entropy can improve the fault localization in the scenario lack of test oracles, serving as an enhancement for fault localization in practice.},
author = {Xiaobo, Yan and Bin, Liu and Shihai, Wang and Dong, An and Feng, Zhu and Yelin, Yang},
doi = {10.1016/j.infsof.2021.106543},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Fault localization,Information entropy,Software debugging,Test oracle,Unlabelled test cases},
month = {jun},
pages = {106543},
publisher = {Elsevier B.V.},
title = {{Efilter: An effective fault localization based on information entropy with unlabelled test cases}},
volume = {134},
year = {2021}
}
@article{Vineesh2021,
abstract = {Design debugging is one of the most important steps in the modern integrated circuits (ICs) development cycle. Simulation-based verification is never sufficient for ensuring design correctness because of its incomplete nature. Formal techniques such as model checking promise to solve this issue through a complete state-space traversal approach. However, because of increasing design complexity, such methods suffer from scalability issues. Guidance-based state-space traversal techniques have been proposed in the past to assist the model checkers in overcoming the complexity bottleneck. Automatically identifying these guidance hints is relatively difficult and requires heuristic-based reasoning procedures. Additionally, to come up with quick fixes during the debug stage, an effective bug localization strategy is needed. In this article, we revisit the paradigm of guidance-based model checking and propose a methodology to improve these guidance generation mechanisms for achieving fine-grained bug localization. In particular, this work proposes a systematic methodology to localize the buggy RTL lines from the erroneous RTL simulation trace. The proposed technique involves the mining of invariant-like assertions from simulation traces. The mined assertions act as probable guidance candidates for the model checking exercise. To identify useful guidance hints from possible ones, we use the Bayesian networks that explore conditional dependence between the various hints at different levels and the target property. These guidance hints are utilized for obtaining possible buggy subregions, which are analyzed via an iterative model checking methodology for fine-grained bug localization. By using the proposed framework, bugs can be localized to within a few lines of RTL description.},
author = {Vineesh, V. S. and Kumar, Binod and Shinde, Rushikesh and Sharma, Neelam and Fujita, Masahiro and Singh, Virendra},
doi = {10.1109/TCAD.2020.3011039},
issn = {19374151},
journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
keywords = {Bayesian networks (BNs),decision tree-based mining,design debugging,formal verification,guidance hints,guided state-space traversal},
month = {may},
number = {5},
pages = {985--998},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Enhanced Design Debugging with Assistance from Guidance-Based Model Checking}},
volume = {40},
year = {2021}
}
@book{Xie2021,
author = {Xie, Xiaoyuan and Xu, Baowen},
booktitle = {Essential Spectrum-based Fault Localization},
doi = {10.1007/978-981-33-6179-9},
pages = {1--172},
publisher = {Springer Singapore},
title = {{Essential Spectrum-based Fault Localization}},
year = {2021}
}
@article{Yang2021,
abstract = {Fault localization techniques are originally proposed to assist in manual debugging by generally producing a rank list of suspicious locations. With the increasing popularity of automated program repair, the fault localization techniques have been introduced to effectively reduce the search space of automated program repair. Unlike developers who mainly focus on the rank information, current automated program repair has two strategies to use the fault localization information: suspiciousness-first algorithm (SFA) based on the suspiciousness accuracy and rank-first algorithm (RFA) relying on the rank accuracy. However, despite the fact that the two different usages are widely adopted by current automated program repair and may result in different repair results, little is known about the impacts of the two strategies on automated program repair. In this paper we empirically compare the performance of SFA and RFA in the context of automated program repair. Specifically, we implement the two strategies and six well-studied fault localization techniques into four state-of-the-art automated program repair tools, and then use these tools to perform repair experiments on 60 real-world bugs from Defects4J. Our study presents a number of interesting findings: RFA outperforms SFA in 70.02% of cases when measured by the number of candidate patches generated before a valid patch is found (NCP), while SFA performs better in parallel repair and patch diversity; the performance of SFA can be improved by increasing the suspiciousness accuracy of fault localization techniques; finally, we use SimFix that deploys SFA to successfully repair four extra Defects4J bugs which cannot be repaired by SimFix originally using RFA. These observations provide a new perspective for future research on the usage and improvement of fault localization in automated program repair.},
author = {Yang, Deheng and Qi, Yuhua and Mao, Xiaoguang and Lei, Yan},
doi = {10.1007/s11704-020-9263-1},
file = {::},
issn = {20952236},
journal = {Frontiers of Computer Science},
keywords = {automated program repair,empirical study,fault localization},
month = {feb},
number = {1},
pages = {151202},
publisher = {Higher Education Press Limited Company},
title = {{Evaluating the usage of fault localization in automated program repair: an empirical study}},
volume = {15},
year = {2021}
}
@phdthesis{Soremekun2021,
author = {Soremekun, Ezekiel Olamide},
title = {{Evidence-driven Testing and Debugging of Software Systems}},
url = {https://publikationen.sulb.uni-saarland.de/handle/20.500.11880/31243},
year = {2021}
}
@article{Wang2020,
abstract = {Background: In modern software systems' maintenance and evolution, how to fix software bugs efficiently and effectively becomes increasingly more essential. A deep understanding of developers'/assignees' familiarity with bugs could help project managers make a proper allotment of maintenance resources. However, to our knowledge, the effects of developer familiarity on bug fixing have not been studied. Aims: Inspired by the understanding of developers'/assignees' familiarity with bugs, we aim to investigate the effects of familiarity on efficiency and effectiveness of bug fixing. Method: Based on evolution history of buggy code lines, we propose three metrics to evaluate the developers'/assignees' familiarity with bugs. Additionally, we conduct an empirical study on 6 well-known Apache Software Foundation projects with more than 9000 confirmed bugs. Results: We observe that (a) familiarity is one of the common factors in cases of bug fixing: the developers are more likely to be assigned to fix the bugs introduced by themselves; (b) familiarity has complex effects on bug fixing: although the developers fix the bugs introduced by themselves more quickly (with high efficiency), they are more likely to introduce future bugs when fixing the current bugs (with worse effectiveness). Conclusion: We put forward the following suggestions: (a) managers should assign some “outsiders” to participate in bug fixing. (b) when developers deal with his own code, managers should assign more maintenance resource (e.g., more inspection) to developers.},
author = {Wang, Chuanqi and Li, Yanhui and Chen, Lin and Huang, Wenchin and Zhou, Yuming and Xu, Baowen},
doi = {10.1016/j.jss.2020.110667},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Bug fixing,Efficiency and effectiveness,Familiarity,Software maintenance and evolution,Software metrics},
month = {nov},
pages = {110667},
publisher = {Elsevier Inc.},
title = {{Examining the effects of developer familiarity on bug fixing}},
volume = {169},
year = {2020}
}
@article{Ramirez-Mora2021,
abstract = {Context: Bug fixing is a frequent and important task in Open Source Software (OSS) development and involves the communication of messages, which can serve for multiple purposes and affect the efficiency and effectiveness of corrective software activities. Objective: This work is aimed at studying the communication functions of bug comments and their associations with fast and complete bug fixing in OSS development. Method: Over 500K comments and 89K bugs of 100 OSS projects were extracted from three Issue Tracking Systems. Six thousand comments were manually tagged to create a corpus of communication functions. The extracted comments were automatically tagged using machine learning algorithms and the corpus of communication functions. Statistical and correlation analyses were performed and the most frequent comments communicated during fast and successful bug fixing were identified. Results: Significant differences in the distribution of comments of fixed and not fixed bugs were found. Variations in the distribution of comments of bugs with different fixing time were also found. Referential comments that provided objective information were found to be the most frequent messages. Results showed that the percentages of conative and emotive comments are greater when bugs are resolved without the requested fixes and when fixes are implemented in a long time. Conclusion: Associations between communication functions and bug fixing exist. The results of this work could be used to improve corrective tasks in OSS development and some other specific linguistic aspects should be studied in detail in OSS communities.},
author = {Ram{\'{i}}rez-Mora, Sandra L. and Oktaba, Hanna and G{\'{o}}mez-Adorno, Helena and Sierra, Gerardo},
doi = {10.1016/j.infsof.2021.106584},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Bug fixing,Communication function,Corrective maintenance,Issue Tracking System,Machine learning,Open Source Software development},
month = {aug},
pages = {106584},
publisher = {Elsevier B.V.},
title = {{Exploring the communication functions of comments during bug fixing in Open Source Software projects}},
volume = {136},
year = {2021}
}
@article{Krasniqi2021,
abstract = {When developers investigate a new bug report, they search for similar previously fixed bug reports and discussion threads attached to them. These discussion threads convey important information about the behavior of the bug including relevant bug-fixing comments. Oftentimes, these discussion threads become extensively lengthy due to the severity of the reported bug. This adds another layer of complexity, especially if relevant bug-fixing comments intermingle with seemingly unrelated comments. To manually detect these relevant comments among various cross-cutting discussion threads can become a daunting task when dealing with high volume of bug reports. To automate this process, our focus is to initially extract and detect comments in the context of query relevance, the use of positive language, and semantic relevance. Then, we merge these comments in the form of a summary for easy understanding. Specifically, we combine Sentiment Analysis and the TextRank Model with the baseline Vector Space Model (VSM). Preliminary findings indicate that bug-fixing comments tend to be positive and there exists a semantic relevance with comments from other cross-cutting discussion threads. The results also indicate that our combined approach improves overall ranking performance against the baseline VSM.},
archivePrefix = {arXiv},
arxivId = {2103.15211},
author = {Krasniqi, Rrezarta},
eprint = {2103.15211},
isbn = {9781665444729},
journal = {CoRR},
month = {mar},
title = {{Extractive Summarization of Related Bug-fixing Comments in Support of Bug Repair}},
url = {https://arxiv.org/abs/2103.15211 http://arxiv.org/abs/2103.15211},
volume = {abs/2103.1},
year = {2021}
}
@inproceedings{Li2021,
abstract = {In this paper, we propose DeepRL4FL, a deep learning fault localization (FL) approach that locates the buggy code at the statement and method levels by treating FL as an image pattern recognition problem. DeepRL4FL does so via novel code coverage representation learning (RL) and data dependencies RL for program statements. Those two types of RL on the dynamic information in a code coverage matrix are also combined with the code representation learning on the static information of the usual suspicious source code. This combination is inspired by crime scene investigation in which investigators analyze the crime scene (failed test cases and statements) and related persons (statements with dependencies), and at the same time, examine the usual suspects who have committed a similar crime in the past (similar buggy code in the training data). For the code coverage information, DeepRL4FL first orders the test cases and marks error-exhibiting code statements, expecting that a model can recognize the patterns discriminating between faulty and non-faulty statements/methods. For dependencies among statements, the suspiciousness of a statement is seen taking into account the data dependencies to other statements in execution and data flows, in addition to the statement by itself. Finally, the vector representations for code coverage matrix, data dependencies among statements, and source code are combined and used as the input of a classifier built from a Convolution Neural Network to detect buggy statements/methods. Our empirical evaluation shows that DeepRL4FL improves the top-1 results over the state-of-the-art statement-level FL baselines from 173.1% to 491.7%. It also improves the top-1 results over the existing method-level FL baselines from 15.0% to 206.3%.},
archivePrefix = {arXiv},
arxivId = {2103.00270},
author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
booktitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
doi = {10.1109/icse43902.2021.00067},
eprint = {2103.00270},
month = {may},
pages = {661--673},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Fault Localization with Code Coverage Representation Learning}},
year = {2021}
}
@inproceedings{Zheng2021,
abstract = {Fault localization is a practical research topic that helps developers identify code locations that might cause bugs in a program. Most existing fault localization techniques are designed for imperative programs (e.g., C and Java) and rely on analyzing correct and incorrect executions of the program to identify suspicious statements. In this work, we introduce a fault localization approach for models written in a declarative language, where the models are not "executed," but rather converted into a logical formula and solved using backend constraint solvers. We present FLACK, a tool that takes as input an Alloy model consisting of some violated assertion and returns a ranked list of suspicious expressions contributing to the assertion violation. The key idea is to analyze the differences between counterexamples, i.e., instances of the model that do not satisfy the assertion, and instances that do satisfy the assertion to find suspicious expressions in the input model. The experimental results show that FLACK is efficient (can handle complex, real-world Alloy models with thousand lines of code within 5 seconds), accurate (can consistently rank buggy expressions in the top 1.9\% of the suspicious list), and useful (can often narrow down the error to the exact location within the suspicious expressions).},
archivePrefix = {arXiv},
arxivId = {2102.10152},
author = {Zheng, Guolong and Nguyen, ThanhVu and Brida, Simon Gutierrez and Regis, German and Frias, Marcelo F. and Aguirre, Nazareno and Bagheri, Hamid},
booktitle = {ICSE 2021 Technical Track},
doi = {10.1109/icse43902.2021.00065},
eprint = {2102.10152},
month = {may},
pages = {637--648},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{FLACK: Counterexample-Guided Fault Localization for Alloy Models}},
year = {2021}
}
@article{Miryeganeh2021,
abstract = {Fault Localization (FL) is an important first step in software debugging and is mostly manual in the current practice. Many methods have been proposed over years to automate the FL process, including information retrieval (IR)-based techniques. These methods localize the fault based on the similarity of the reported bug report and the source code. Newer variations of IR-based FL (IRFL) techniques also look into the history of bug reports and leverage them during the localization. However, all existing IRFL techniques limit themselves to the current project's data (local data). In this study, we introduce Globug, which is an IRFL framework consisting of methods that use models pre-trained on the global data (extracted from open-source benchmark projects). In Globug, we investigate two heuristics: (a) the effect of global data on a state-of-the-art IR-FL technique, namely BugLocator, and (b) the application of a Word Embedding technique (Doc2Vec) together with global data. Our large scale experiment on 51 software projects shows that using global data improves BugLocator on average 6.6% and 4.8% in terms of MRR (Mean Reciprocal Rank) and MAP (Mean Average Precision), with over 14% in a majority (64% and 54% in terms of MRR and MAP, respectively) of the cases. This amount of improvement is significant compared to the improvement rates that five other state-of-the-art IRFL tools provide over BugLocator. In addition, training the models globally is a one-time offline task with no overhead on BugLocator's run-time fault localization. Our study, however, shows that a Word Embedding-based global solution did not further improve the results.},
archivePrefix = {arXiv},
arxivId = {2101.05862},
author = {Miryeganeh, Nima and Hashtroudi, Sepehr and Hemmati, Hadi},
doi = {10.1016/j.jss.2021.110961},
eprint = {2101.05862},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Automated Fault Localization,Doc2Vec,Global training,Information Retrieval,TF.IDF,Word Embedding},
month = {jul},
pages = {110961},
publisher = {Elsevier Inc.},
title = {{GloBug: Using global data in Fault Localization}},
volume = {177},
year = {2021}
}
@article{Peng2020,
author = {Peng, Xin},
doi = {10.1109/MC.2019.2957845},
file = {::},
issn = {15580814},
journal = {Computer},
month = {feb},
number = {2},
pages = {4--5},
publisher = {IEEE Computer Society},
title = {{Helping Developers Analyze and Debug Industrial Microservice Systems}},
volume = {53},
year = {2020}
}
@article{Yuan2020,
abstract = {Context: Distributed systems are the backbone of today's computing ecosystems. Debugging distributed bugs is crucial and challenging. There are still many unknowns about debugging real-world distributed bugs, especially through system logs. Objective: This paper aims to provide a comprehensive study of how system logs can help diagnose and fix distributed bugs in practice. Method: The study was carried out with three core research questions (RQs): How to identify failures in distributed bugs through logs? How to find and utilize bug-related log entries to figure out the root causes? How are distributed bugs fixed and how are logs and patches related? To answer these questions, we studied 106 real-world distributed bugs randomly sampled from five widely used distributed systems, and manually checked the bug report, the log, the patch, the source code and other related information for each of these bugs. Results: Seven findings are observed and the main findings include: (1) For only about half of the distributed bugs, the failures are indicated by FATAL or ERROR log entries. FATAL are not always fatal, and INFO could be fatal. (2) For more than half of the studied bugs, root-cause diagnosis relies on log entries that are not part of the failure symptoms. (3) One third of the studied bugs are fixed by eliminating end symptoms instead of root causes. Finally, a distributed bug dataset with the in-depth analysis has been released to the research community. Conclusion: The findings in our study reveal the characteristics of distributed bugs, the differences from debugging single-machine system bugs, and the usages and limitations of existing logs. Our study also provides guidance and opportunities for future research on distributed bug diagnosis, fixing, and log analysis and enhancement.},
author = {Yuan, Wei and Lu, Shan and Sun, Hailong and Liu, Xudong},
doi = {10.1016/j.infsof.2019.106234},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Distributed systems,Issue reports,Software bug analysis,System logs},
month = {mar},
publisher = {Elsevier B.V.},
title = {{How are distributed bugs diagnosed and fixed through system logs?}},
volume = {119},
year = {2020}
}
@article{Prokop2020,
abstract = {Simulations and games bring the possibility to research complex processes of managerial decision-making. However, this modern field requires adequate methodological procedures. Many authors recommend the use of a combination of concurrent think-aloud (CTA) or retrospective think-aloud (RTA) with eye-tracking to investigate cognitive processes such as decision-making. Nevertheless, previous studies have little or no consideration of the possible differential impact of both think-aloud methods on data provided by eye-tracking. Therefore, the main aim of this study is to compare and assess if and how these methods differ in terms of their impact on eye-tracking. The experiment was conducted for this purpose. Participants were 14 managers who played a specific simulation game with CTA use and 17 managers who played the same game with RTA use. The results empirically prove that CTA significantly distorts data provided by eye-tracking, whereas data gathered when RTA is used, provide independent pieces of evidence about the participants' behavior. These findings suggest that RTA is more suitable for combined use with eye-tracking for the purpose of the research of decision-making in the game environment.},
author = {Prokop, Michal and Pilař, Ladislav and Tich{\'{a}}, Ivana},
doi = {10.3390/s20102750},
file = {::},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Decision-making,Eye-tracking,Games,Simulation game,Think-aloud},
number = {10},
pages = {2750},
pmid = {32408507},
publisher = {MDPI AG},
title = {{Impact of think-aloud on eye-tracking: A comparison of concurrent and retrospective think-aloud for research on decision-making in the game environment}},
volume = {20},
year = {2020}
}
@inproceedings{Insa2012,
abstract = {One of the most automatic debugging techniques is Algorithmic Debugging because it allows us to debug a program without the need to inspect the source code. In order to find a bug, an algorithmic debugger asks questions to the programmer about the correctness of subcomputations in an execution. Reducing the number and complexity of these questions is an old objective in this field. Recently, an strategy for algorithmic debuggers that minimizes the number of questions has been released. This new strategy is called Optimal Divide and Query and, provided that all questions can be answered, it finds any bug in the source code with a minimum set of questions. In this work we discuss the implementation of such a strategy in different algorithmic debugging architectures. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Insa, David and Silva, Josep},
booktitle = {Electronic Notes in Theoretical Computer Science},
doi = {10.1016/j.entcs.2011.12.005},
file = {::},
issn = {15710661},
keywords = {Algorithmic Debugging,Debugging},
month = {may},
pages = {47--60},
title = {{Implementation of an optimal strategy for Algorithmic Debugging}},
volume = {282},
year = {2012}
}
@article{Zhang2021,
abstract = {Many fault localization approaches recently utilize deep learning to learn an effective localization model showing a fresh perspective with promising results. However, localization models are generally learned from class imbalance datasets; that is, the number of failing test cases is much fewer than passing test cases. It may be highly susceptible to affect the accuracy of learned localization models. Thus, in this paper, we explore using data resampling to reduce the negative effect of the imbalanced class problem and improve the accuracy of learned models of deep-learning-based fault localization. Specifically, for deep-learning-based fault localization, its learning feature may require duplicate essential data to enhance the weak but beneficial experience incurred by the class imbalance datasets. We leverage the property of test cases (i.e., passing or failing) to identify failing test cases as the duplicate essential data and propose an iterative oversampling approach to resample failing test cases for producing a class balanced test suite. We apply the test case resampling to representative localization models using deep learning. Our empirical results on eight large-sized programs with real faults and four large-sized programs with seeded faults show that the test case resampling significantly improves fault localization effectiveness.},
author = {Zhang, Zhuo and Lei, Yan and Mao, Xiaoguang and Yan, Meng and Xu, Ling and Wen, Junhao},
doi = {10.1002/smr.2312},
issn = {20477481},
journal = {Journal of Software: Evolution and Process},
keywords = {debugging,deep learning,fault localization,neural networks,resampling},
month = {mar},
number = {3},
publisher = {John Wiley and Sons Ltd},
title = {{Improving deep-learning-based fault localization with resampling}},
volume = {33},
year = {2021}
}
@inproceedings{Gouveia2012,
abstract = {The automatic matching of entities between information repositories is essential for aligning and ontology mediation processes, but there is great ambiguity when adopting it. Data integration between repositories demands great quality of ontology alignments, and as such ambiguous correspondences must be identified and corrected beforehand. Debugging is even more relevant if the process is systematic and complete, thus allowing its formalisation and implementation and promoting its acceptance. In this paper we address the analysis and systematization of the ontology alignment debugging process, proposing the characterization of the ontology matching scenarios through ten dimensions. The resulting scenarios are categorized as Simple and Composed depending on the associated actions taken for solving the ambiguity. We suggest evolving the alignment process into an iterative matching-debugging process, where the resulting ambiguous scenarios and correction strategies are fed into the matching algorithms for supporting the resolution of the alignment problems. {\textcopyright} 2012 IEEE.},
author = {Gouveia, Alexandre and Silva, Nuno and Rocha, Jo{\~{a}}o and Martins, Paulo},
booktitle = {Proceedings - International Workshop on Database and Expert Systems Applications, DEXA},
doi = {10.1109/DEXA.2012.37},
isbn = {9780769548012},
issn = {15294188},
keywords = {alignment,correspondences,debugging,ontology},
pages = {254--258},
title = {{Iterative ontology alignment debugging using a scenario- and strategy-driven approach}},
year = {2012}
}
@phdthesis{Habib2021,
author = {Habib, Andrew},
title = {{Learning to Find Bugs in Programs and their Documentation.}},
url = {http://tuprints.ulb.tu-darmstadt.de/17377/ http://tuprints.ulb.tu-darmstadt.de/17377/%0Ahttps://nbn-resolving.org/urn:nbn:de:tuda-tuprints-173778%0Ahttp://d-nb.info/1226935419},
year = {2021}
}
@inproceedings{Kumar2021,
author = {Kumar, Amruth N.},
booktitle = {AIED 2021: Artificial Intelligence in Education},
doi = {10.1007/978-3-030-78270-2_39},
pages = {219--223},
title = {{Long Term Retention of Programming Concepts Learned Using Tracing Versus Debugging Tutors}},
year = {2021}
}
@article{Mcdonald2016,
abstract = {We report the results of a study comparing two concurrent think-aloud approaches for usability testing: the classic think-aloud (CTA) and an interactive think-aloud (ITA). The think-alouds were compared in respect of task performance and usability problem data. We also analyse the utility of the interventions used within the ITA in eliciting useful participant utterances. The most useful interventions were those focused on seeking explanations and opinions; these generated more utterances about user difficulties. Requests for clarifications, particularly about actions, resulted in fewer useful utterances: participants responded with simple procedural descriptions. In comparing the CTA and ITA, we found no differences in the number of successfully completed tasks, but the ITA did elongate the test session. The ITA led to the detection of more usability problems overall, and a greater number of causal explanations. However, the ITA produced more low-severity problems than the CTA.},
author = {Mcdonald, Sharon and Zhao, Tingting and Edwards, Helen M.},
doi = {10.1093/iwc/iwv014},
issn = {09535438},
journal = {Interacting with Computers},
keywords = {classic think-aloud,concurrent think-aloud,interactive think-aloud,usability evaluation,usability testing,user studies},
month = {may},
number = {3},
pages = {387--403},
publisher = {Oxford University Press},
title = {{Look who's talking: Evaluating the utility of interventions during an interactive think-aloud}},
volume = {28},
year = {2016}
}
@article{Zhu2021,
abstract = {In this work, we study how the authorship of code affects bug-fixing commits using the SStuBs dataset, a collection of single-statement bug fix changes in popular Java Maven projects. More specifically, we study the differences in characteristics between simple bug fixes by the original author -- that is, the developer who submitted the bug-inducing commit -- and by different developers (i.e., non-authors). Our study shows that nearly half (i.e., 44.3%) of simple bugs are fixed by a different developer. We found that bug fixes by the original author and by different developers differed qualitatively and quantitatively. We observed that bug-fixing time by authors is much shorter than that of other developers. We also found that bug-fixing commits by authors tended to be larger in size and scope, and address multiple issues, whereas bug-fixing commits by other developers tended to be smaller and more focused on the bug itself. Future research can further study the different patterns in bug-fixing and create more tailored tools based on the developer's needs.},
archivePrefix = {arXiv},
arxivId = {2103.11894},
author = {Zhu, Wenhan and Godfrey, Michael W},
eprint = {2103.11894},
isbn = {9781728187105},
journal = {CoRR},
keywords = {Index Terms-SStuBs,Open source,Open source development,bug fix,empirical software engineering},
month = {mar},
title = {{Mea culpa: How developers fix their own simple bugs differently from other developers}},
url = {https://arxiv.org/abs/2103.11894 https://anonymous.4open.science/r/344cf208-ea32-49f4-90fe-59bdb6e5d7fe/ http://arxiv.org/abs/2103.11894},
volume = {abs/2103.1},
year = {2021}
}
@article{Parizi2018,
abstract = {Microservices become a fast growing and popular architectural style based on service-oriented development. One of the major advantages using component-based approaches is to support reuse. In this paper, we present a study of microservices and how these systems are related to the traditional abstract models of component-based systems. This research focuses on the core properties of microservices including their scalability, availability and resilience, consistency, coupling and cohesion, and data storage capability, while highlighting their limitations and challenges in relation to components. To support our study, we investigated the existing literature and provided potential directions and interesting points in this growing field of research. As a result, using microservices as components is promising and would be a good mechanism for building applications that were used to be built with component-based approaches.},
archivePrefix = {arXiv},
arxivId = {1805.11757},
author = {Parizi, Reza M.},
eprint = {1805.11757},
journal = {CoRR},
title = {{Microservices as an Evolutionary Architecture of Component-Based Development: A Think-aloud Study}},
url = {http://arxiv.org/abs/1805.11757},
volume = {abs/1805.1},
year = {2018}
}
@inproceedings{Yu2008,
abstract = {The paper uses data mining approaches to classify bug types and excavate debug strategy association rules for Web-based applications. Chi-square algorithm is used to extract bug features, and SVM to model bug classifier achieving more than 70% predication accuracy on average. Debug strategy association rules accumulate bug fixing knowledge and experiences regarding to typical bug types, and can be applied repeatedly, thus improving the bug fixing efficiency. With 575 training data, three debug strategy association rules are unearthed. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {Yu, Lian and Kong, Changzhu and Xu, Lei and Zhao, Jingtao and Zhang, Huihui},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-88192-6_40},
isbn = {3540881913},
issn = {16113349},
keywords = {Association rule,Bug classification,Bug mining,Chi-square algorithm,Debug strategy,SVM},
pages = {427--434},
publisher = {Springer Verlag},
title = {{Mining bug classifier and debug strategy association rules for web-based applications}},
volume = {5139 LNAI},
year = {2008}
}
@article{Dutta2021,
abstract = {Fault localization techniques aim to localize faulty statements using the information gathered from both passed and failed test cases. We present a mutation-based fault localization technique called MuSim. MuSim identifies the faulty statement based on its computed proximity to different mutants. We study the performance of MuSim by using four different similarity metrics. To satisfactorily measure the effectiveness of our proposed approach, we present a new evaluation metric called Mut_Score. Based on this metric, on an average, MuSim is 33.21% more effective than existing fault localization techniques such as DStar, Tarantula, Crosstab, Ochiai.},
author = {Dutta, Arpita and Jha, Amit and Mall, Rajib},
doi = {10.1142/S0218194021500212},
issn = {02181940},
journal = {International Journal of Software Engineering and Knowledge Engineering},
keywords = {Debugging,cosine similarity,fault localization,mutation testing,program analysis,regression testing},
month = {may},
number = {5},
pages = {725--744},
publisher = {World Scientific},
title = {{MuSim: Mutation-based Fault Localization Using Test Case Proximity}},
volume = {31},
year = {2021}
}
@article{Shimari2021,
abstract = {Logging is an important feature of a software system to record run-time information. Detailed logging allows developers to collect run-time information in situations where they cannot use an interactive debugger, such as continuous integration and web application server cases. However, extensive logging leads to larger execution traces because few instructions can be repeated many times. This paper presents our tool NOD4J, which monitors a Java program's execution within limited storage space constraints and annotates the source code with observed values in an HTML format. Developers can easily investigate the execution and share the report on a web server. We show two examples that our tool can debug defects using incomplete execution traces.},
author = {Shimari, Kazumasa and Ishio, Takashi and Kanda, Tetsuya and Ishida, Naoto and Inoue, Katsuro},
doi = {10.1016/j.scico.2021.102630},
file = {::},
issn = {01676423},
journal = {Science of Computer Programming},
keywords = {Dynamic analysis,Logging,Software visualization},
month = {jun},
pages = {102630},
publisher = {Elsevier B.V.},
title = {{NOD4J: Near-omniscient debugging tool for Java using size-limited execution trace}},
volume = {206},
year = {2021}
}
@inproceedings{Fan2021,
abstract = {Subtle patterns in users' think-aloud (TA) verbalizations and speech features are shown to be telltale signs of User Experience (UX) problems. However, such patterns were uncovered among young adults. Whether such patterns apply for older adults remains unknown. We conducted TA usability testing with older adults using physical and digital products. We analyzed their verbalizations, extracted speech features, identifed UX problems, and uncovered the patterns that indicate UX problems. Our results show that when older adults encounter problems, their verbalizations tend to include observations (remarks), negations, question words and words with negative sentiments; and their voices tend to include high loudness, high pitch and high speech rate. We compare these subtle patterns with those of young adults uncovered in recent studies and discuss the implications of these patterns for the design of Human-AI collaborative UX analysis tools to better pinpoint UX problems.},
author = {Fan, Mingming and Zhao, Qiwen and Tibdewal, Vinita},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/3411764.3445680},
isbn = {9781450380966},
keywords = {Ai-assisted ux analysis,Elderly,Human-ai collaboration for ux analysis,Older adults,Remote usability testing,Seniors,Speech features,Think-aloud,Usability testing,Ux problems,Verbalization},
month = {may},
pages = {358:1--358:13},
publisher = {Association for Computing Machinery},
title = {{Older adults' think-aloud verbalizations and speech features for identifying user experience problems}},
year = {2021}
}
@inproceedings{Izu2017,
abstract = {Abstraction is a core skill for both programming and problem solving, however it is also a challenge for many students to develop a correct understanding of abstract concepts, such as program behaviour, which causes them to struggle with both introductory and advanced programming courses. Thus, evaluating students' ability to reason about programs should be an important topic for CS education. We use a think-aloud study to record and analyse the strategies students apply to reason about program behaviour within the context of reversibility. Reversibility is a property of a program or function that indicates it could be brought back to its original state. Reasoning about reversibility requires students to have a mental model of the state, thus they should reason about program behaviour as a whole, compared with reasoning about concrete cases using testing and tracing. We have identified four strategies used by students to complete the reversibility task, which we have named as algorithm decomposition, input analysis, abstraction and inductive testing. Although 70% of students successful identified reversibility in 2 of the 3 exercises, most students fail to correctly reason about reversibility in an exercise involving a seemingly simple conditional statement.},
author = {Izu, Cruz and Pope, Cheryl and Weerasinghe, Amali},
booktitle = {Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},
doi = {10.1145/3059009.3059036},
isbn = {9781450347044},
issn = {1942647X},
keywords = {Programming,Reasoning,Reversibility,State,Think-aloud},
month = {jun},
pages = {305--310},
publisher = {Association for Computing Machinery},
title = {{On the ability to reason about program behaviour: A think-aloud study}},
volume = {Part F1286},
year = {2017}
}
@inproceedings{Cosman2020,
abstract = {As dynamically-typed languages grow in popularity, especially among beginning programmers, there is an increased need to pinpoint their defects. Localization for novice bugs can be ambiguous: not all locations formally implicated are equally useful for beginners. We propose a scalable fault localization approach for dynamic languages that is helpful for debugging and generalizes to handle a wide variety of errors commonly faced by novice programmers.We base our approach on a combination of static, dynamic, and contextual features, guided by machine learning. We evaluate on over 980,000 diverse real user interactions across four years from the popular PythonTutor.com website, which is used both in classes and by non-traditional learners. We find that our approach is scalable, general, and quite accurate: up to 77% of these historical novice users would have been helped by our top-three responses, compared to 45% for the default interpreter. We also conducted a human study: participants preferred our approach to the baseline (p = 0.018), and found it additionally useful for bugs meriting multiple edits.},
author = {Cosman, Benjamin and Endres, Madeline and Sakkas, Georgios and Medvinsky, Leon and Yang, Yao Yuan and Jhala, Ranjit and Chaudhuri, Kamalika and Weimer, Westley},
booktitle = {Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},
doi = {10.1145/3328778.3366860},
file = {::},
isbn = {9781450367936},
issn = {1942647X},
month = {feb},
pages = {1047--1053},
publisher = {Association for Computing Machinery},
title = {{PABLO: Helping novices debug python code through data-driven fault localization}},
year = {2020}
}
@inproceedings{Jayathirtha2020,
abstract = {Much attention has focused on student learning while making physical computational artifacts such as robots or electronic textiles, but little is known about how students engage with the hardware and software debugging issues that often arise. In order to better understand students' debugging strategies and practices, we conducted and video-recorded eight think-aloud sessions ($\sim$45 minutes each) of high school student pairs debugging electronic textiles projects with researcher-designed programming and circuitry/crafting bugs. We analyzed each video to understand pairs' debugging strategies and practices in navigating the multi-representational problem space. Our findings reveal the importance of employing system-level strategies while debugging physical computing systems, and of coordinating between various components of physical computing systems, for instance between the physical artifact, representations on paper, and the onscreen programming environment. We discuss the implications of our findings for future research and designing instruction and tools for learning with and debugging physical computing systems.},
author = {Jayathirtha, Gayithri and Fields, Deborah and Kafa, Yasmin},
booktitle = {Computer-Supported Collaborative Learning Conference, CSCL},
isbn = {9781732467262},
issn = {15734552},
keywords = {Debugging,Electronic textiles,High school computing learning,Physical computing,Think-aloud protocol},
pages = {1047--1054},
title = {{Pair debugging of electronic textiles projects: Analyzing think-aloud protocols for high school students' strategies and practices while problem solving}},
url = {https://repository.isls.org/handle/1/6292},
volume = {2},
year = {2020}
}
@inproceedings{Jayathirtha2021a,
abstract = {Comprehending programs is key to learning programming. Previous studies highlight novices' naive approaches to comprehending the structural, functional, and behavioral aspects of programs. And yet, with the majority of them examining on-screen programming environments, we barely know about program comprehension within physical computing-a common K-12 programming context. In this study, we qualitatively analyzed think-aloud interview videos of 22 high school students individually comprehending a given text-based Arduino program while interacting with its corresponding functional physical artifact to answer two questions: 1) How do novices comprehend the given text-based Arduino pro-gram? And, 2) What role does the physical artifact play in program comprehension? We found that novices mostly approached the program bottom-up, initially comprehending structural and later functional aspects, along different granularities. The artifact provided two distinct modes of engagement, active and interactive, that supported the program's structural and functional comprehension. However, behavioral comprehension i.e. understanding program execution leading to the observed outcome was inaccessible to many. Our findings extend program comprehension literature in two ways: (a) it provides one of the very few accounts of high school students' code comprehension in a physical computing context , and, (b) it highlights the mediating role of physical artifacts in program comprehension. Further, they point directions for future pedagogical and tool designs within physical computing to better support students' distributed program comprehension. CCS CONCEPTS • Applied computing → Interactive learning environments.},
author = {Jayathirtha, Gayithri and Kafai, Yasmin B.},
booktitle = {ITiCSE '21: Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
doi = {10.1145/3430665.3456371},
file = {::},
keywords = {electronic textiles,physical computing,program comprehension,secondary education},
month = {jun},
number = {1},
pages = {143--149},
publisher = {Association for Computing Machinery (ACM)},
title = {{Program Comprehension with Physical Computing: A Structure, Function, and Behavior Analysis of Think-Alouds with High School Students}},
url = {https://doi.org/10.1145/3430665.3456371},
year = {2021}
}
@inproceedings{Jayathirtha2021,
abstract = {Comprehending programs is key to learning programming. Previous studies highlight novices' naive approaches to comprehending the structural, functional, and behavioral aspects of programs. And yet, with the majority of them examining on-screen programming environments, we barely know about program comprehension within physical computing-a common K-12 programming context. In this study, we qualitatively analyzed think-aloud interview videos of 22 high school students individually comprehending a given text-based Arduino program while interacting with its corresponding functional physical artifact to answer two questions: 1) How do novices comprehend the given text-based Arduino pro-gram? And, 2) What role does the physical artifact play in program comprehension? We found that novices mostly approached the program bottom-up, initially comprehending structural and later functional aspects, along different granularities. The artifact provided two distinct modes of engagement, active and interactive, that supported the program's structural and functional comprehension. However, behavioral comprehension i.e. understanding program execution leading to the observed outcome was inaccessible to many. Our findings extend program comprehension literature in two ways: (a) it provides one of the very few accounts of high school students' code comprehension in a physical computing context , and, (b) it highlights the mediating role of physical artifacts in program comprehension. Further, they point directions for future pedagogical and tool designs within physical computing to better support students' distributed program comprehension. CCS CONCEPTS • Applied computing → Interactive learning environments.},
author = {Jayathirtha, Gayithri and Kafai, Yasmin B.},
booktitle = {ITiCSE '21: Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
doi = {10.1145/3430665.3456371},
file = {:C\:/Users/geryxyz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jayathirtha, Kafai - 2021 - Program Comprehension with Physical Computing A Structure, Function, and Behavior Analysis of Think-Alouds w.pdf:pdf},
keywords = {electronic textiles,physical computing,program comprehension,secondary education},
month = {jun},
number = {1},
pages = {143--149},
publisher = {Association for Computing Machinery (ACM)},
title = {{Program Comprehension with Physical Computing: A Structure, Function, and Behavior Analysis of Think-Alouds with High School Students}},
url = {https://doi.org/10.1145/3430665.3456371},
year = {2021}
}
@misc{Shchekotykhin,
abstract = {Debugging is an important prerequisite for the wide-spread application of ontologies, especially in areas that rely upon everyday users to create and maintain knowledge bases, such as the Semantic Web. Most recent approaches use diagnosis methods to identify sources of inconsistency. However, in most debugging cases these methods return many alternative diagnoses, thus placing the burden of fault localization on the user. This paper demonstrates how the target diagnosis can be identified by performing a sequence of observations, that is, by querying an oracle about entailments of the target ontology. We exploit probabilities of typical user errors to formulate information theoretic concepts for query selection. Our evaluation showed that the suggested method reduces the number of required observations compared to myopic strategies. {\textcopyright} 2010 Springer-Verlag.},
author = {Shchekotykhin, Kostyantyn M. and Friedrich, Gerhard},
doi = {10.1007/978-3-642-17746-0_44},
file = {:C\:/Users/geryxyz/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shchekotykhin, Friedrich - 2010 - Query strategy for sequential ontology debugging.pdf:pdf},
isbn = {364217745X},
issn = {03029743},
number = {PART 1},
pages = {696--712},
title = {{Query strategy for sequential ontology debugging}},
url = {http://arxiv.org/abs/1004.5339 http://arxiv.org/abs/1107.4303}
}
@article{Krasniqi2021a,
abstract = {In practice, developers search for related earlier bugs and their associated discussion threads when faced with a new bug to repair. Typically, these discussion threads consist of comments and even bug-fixing comments intended to capture clues for facilitating the investigation and root cause of a new bug report. Over time, these discussions can become extensively lengthy and difficult to understand. Inevitably, these discussion threads lead to instances where bug-fixing comments intermingle with seemingly-unrelated comments. This task, however, poses further challenges when dealing with high volumes of bug reports. Large software systems are plagued by thousands of bug reports daily. Hence, it becomes time-consuming to investigate these bug reports efficiently. To address this gap, this paper builds a ranked-based automated tool that we refer it to as RETRORANK. Specifically, RETRORANK recommends bug-fixing comments from issue tracking discussion threads in the context of user query relevance, the use of positive language, and semantic relevance among comments. By using a combination of Vector Space Model (VSM), Sentiment Analysis (SA), and the TextRank Model (TR) we show how that past fixed bugs and their associated bug-fixing comments with relatively positive sentiments can semantically connect to investigate the root cause of a new bug. We evaluated our approach via a synthetic study and a user study. Results indicate that RETRORANK significantly improved performance when compared to the baseline VSM.},
archivePrefix = {arXiv},
arxivId = {2105.11525},
author = {Krasniqi, Rrezarta},
eprint = {2105.11525},
journal = {CoRR},
title = {{Recommending Bug-fixing Comments from Issue Tracking Discussions in Support of Bug Repair}},
url = {https://arxiv.org/abs/2105.11525 http://arxiv.org/abs/2105.11525},
volume = {abs/2105.1},
year = {2021}
}
@inproceedings{Alhadreti2018,
abstract = {This paper presents the results of a study that compared three think-aloud methods: concurrent think-aloud, retrospective think-aloud, and a hybrid method. The three methods were compared through an evaluation of a library website, which involved four points of comparison: task performance, participants' experiences, usability problems discovered, and the cost of employing the methods. The results revealed that the concurrent method outperformed both the retrospective and the hybrid methods in facilitating successful usability testing. It detected higher numbers of usability problems than the retrospective method, and produced output comparable to that of the hybrid method. The method received average to positive ratings from its users, and no reactivity was observed. Lastly, this method required much less time on the evaluator's part than did the other two methods, which involved double the testing and analysis time.},
author = {Alhadreti, Obead and Mayhew, Pam},
booktitle = {Conference on Human Factors in Computing Systems - Proceedings},
doi = {10.1145/3173574.3173618},
isbn = {9781450356206},
keywords = {Human-computer interaction,Think-aloud protocols,Usability testing,User experiences,User studies},
month = {apr},
pages = {44},
publisher = {Association for Computing Machinery},
title = {{Rethinking thinking aloud: A comparison of three think-aloud protocols}},
volume = {2018-April},
year = {2018}
}
@article{Siddiq2017,
abstract = {Collaborative problem solving (ColPS) skills are considered crucial to succeed in work, education, and life in a knowledge-rich society. Nevertheless, research on the assessment of ColPS is at its initial stage; specifically, assessments of synchronous student-student ColPS in digital environments have been scarcely investigated, and there is an ample need for valid and reliable assessments to measure ColPS. The present study attempts to fill this gap by proposing a novel ColPS task and investigating students' ColPS skills with the help of think-aloud protocols while they were taking an online performance-based test. The task was developed on the basis of a ColPS framework, and principles emphasized in the research literature on students' interaction, collaboration, and problem solving were implemented. A real-world problem mimicking a common teaching and learning situation formed the context of this task. The empirical evidence obtained from the think-aloud protocols of eleven Norwegian students displayed the strengths and weaknesses of the task, and strengthened the feasibility to assess ColPS. Implications for the future design of ColPS tasks are discussed.},
author = {Siddiq, Fazilat and Scherer, Ronny},
doi = {10.1016/j.chb.2017.08.007},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Assessment and teaching of 21st century skills (AT,Collaborative problem solving,Computer-based assessment,Educational technology,Think-aloud protocol},
month = {nov},
pages = {509--525},
publisher = {Elsevier Ltd},
title = {{Revealing the processes of students' interaction with a novel collaborative problem solving task: An in-depth analysis of think-aloud protocols}},
volume = {76},
year = {2017}
}
@article{Salmeron2017,
abstract = {When students solve problems on the Internet, they have to find a balance between quickly scanning large sections of information in web pages and deeply processing those that are relevant for the task. We studied how high school students articulate scanning and deeper processing of information while answering questions using a Wikipedia document, and how their reading comprehension skills and the question type interact with these processes. By analyzing retrospective think-aloud protocols and eye-tracking measures, we found that scanning of information led to poor hypertext comprehension, while deep processing of information produced better performance, especially in location questions. This relationship between scanning, deep processing, and performance was qualified by reading comprehension skills in an unexpected way: Scanning led to lower performance especially for good comprehenders, while the positive effect of deep processing was independent of reading comprehension skills. We discussed the results in light of our current knowledge of Internet problem solving.},
author = {Salmer{\'{o}}n, L. and Naumann, J. and Garc{\'{i}}a, V. and Fajardo, I.},
doi = {10.1111/jcal.12152},
issn = {13652729},
journal = {Journal of Computer Assisted Learning},
keywords = {Internet problem solving,cued retrospective think aloud,eye tracking,reading comprehension},
month = {jun},
number = {3},
pages = {222--233},
publisher = {Blackwell Publishing Ltd},
title = {{Scanning and deep processing of information in hypertext: an eye tracking and cued retrospective think-aloud study}},
volume = {33},
year = {2017}
}
@article{Moezi2021,
abstract = {This paper's main purpose is to present a fault detection and isolation approach in the circuits employing a convolutional neural network and spectrograms. Monte Carlo analysis is performed for each of the existing faults, and several sample signals are generated. Then, spectrograms for one-dimensional output signals are calculated by the short-time discrete Fourier transform, and the convolutional neural network is trained using the spectrograms. The contribution of this paper is twofold. First, we suggest the power spectrogram to generate the features and apply them to the convolutional neural network. Second, usually, more than one fault occurs in circuit elements. So we study the simultaneous faults, which are the most challenging faults to be detected and isolated. Simulation results show that the proposed method has better accuracy than the existing methods from literature, and the computational time and the rate of false alarms have also reduced.},
author = {Moezi, Alireza and Kargar, Seyed Mohamad},
doi = {10.1016/j.compeleceng.2021.107162},
issn = {00457906},
journal = {Computers and Electrical Engineering},
keywords = {Analog circuits,Concurrent faults,Convolutional neural network,Deep learning,Fault detection,Fault isolation,Spectrogram Images},
month = {jun},
pages = {107162},
publisher = {Elsevier Ltd},
title = {{Simultaneous fault localization and detection of analog circuits using deep learning approach}},
volume = {92},
year = {2021}
}
@misc{Yang2021a,
author = {Yang, Bo and Yu, Qian and Liu, Huai and He, Yuze and Liu, Chao},
booktitle = {Frontiers of Computer Science},
doi = {10.1007/s11704-019-9176-z},
issn = {20952236},
month = {feb},
number = {1},
pages = {151203},
publisher = {Higher Education Press Limited Company},
title = {{Software debugging analysis based on developer behavior data}},
volume = {15},
year = {2021}
}
@article{Maru2021,
abstract = {Software failure is inevitable with the increase in scale and complexity of the software. Existing fault localization techniques based on neural networks take statement coverage information and test case execution results into account to train the network. In this paper, we propose an effective approach for fault localization based on back-propagation neural network which utilizes branch and function coverage information along with test case execution results to train the network. We investigated our approach using Siemens suite. Our experimental result shows that our proposed approach performs on average 23.50–44.27% better than existing fault localization techniques.},
author = {Maru, Abha and Dutta, Arpita and Kumar, K. Vinod and Mohapatra, Durga Prasad},
doi = {10.1007/s12065-019-00318-2},
file = {::},
issn = {18645917},
journal = {Evolutionary Intelligence},
keywords = {Back-propagation neural network (BPNN),Clause,Debugging,Failed test case,Fault localization,Predicate,Successful test case},
month = {mar},
number = {1},
pages = {87--104},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Software fault localization using BP neural network based on function and branch coverage}},
volume = {14},
year = {2021}
}
@article{Ghosh2021,
abstract = {Context: In the field of software engineering, the most complex and time consuming activity is fault-finding. Due to increasing size and complexity of software, there is a necessity of automated fault detection tool which can detect fault with minimal human intervention. A programmer spends a lot of time and effort on software fault localization. Various Spectrum Based Fault Localization (SBFL) techniques have already been developed to automate the fault localization in single-fault software. But, there is a scarcity of fault localization technique for multi-fault software. In our study, we have found that pure SBFL is not always sufficient for effective fault localization in multi-fault programs. Objective: To address the above challenge, we propose an automated framework using Chaos-based Genetic Algorithm for Multi-fault Localization (CGAML) based on SBFL technique. Methods: Traditional Genetic Algorithm (GA) sometimes stuck in local optima, and it takes more time to converge. Different chaos mapping functions have been applied to GA for better performance. We have used logistic mapping function to achieve chaotic sequence. The proposed technique CGAML first calculates the suspiciousness score for each program statement and then assigns ranks according to that score. The statements having smaller rank means there is a high probability of the statements to be faulty. Results: Five open-source benchmark programs are tested to evaluate the efficiency of CGAML technique. The experimental results show CGAML gives better results for both single-fault and multi-fault programs in comparison with existing spectrum-based fault localization techniques. Conclusion: EXAM metric is used to compare the performance of our proposed technique with other existing techniques. Smaller EXAM score denotes the higher accuracy of the technique. The proposed framework generates smaller EXAM score in comparison with other existing techniques. We found that, overall CGAML works on an average 8.5% better than GA for both single-fault and multi-fault software.},
author = {Ghosh, Debolina and Singh, Jagannath},
doi = {10.1016/j.infsof.2021.106512},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Chaotic GA,Debugging,Fault localization,Genetic Algorithm,Software testing,Spectrum-based debugging},
month = {may},
pages = {106512},
publisher = {Elsevier B.V.},
title = {{Spectrum-based multi-fault localization using Chaotic Genetic Algorithm}},
volume = {133},
year = {2021}
}
@inproceedings{Piplani2018,
abstract = {In this paper, we propose a test and debug methodology to support at-speed testing of the high-speed mixed signal JESD204B Receiver Physical Layer (JESD204B Rx PHY) serial interface in 28nm Silicon-On-Insulator (SOI) technology. A mixed signal Built-in-Self-Test (BIST) is implemented to target testing of both the 2.4GHz and 60MHz mixed signal domains of the PHY. The BIST consists of an analog pattern generator which injects 2.4Gbps JESD204B compliant differential serial data, an auto-clock phase rotation to stress Clock Data Recovery (CDR) block and a checker to validate the 60MHz parallel digital data received at the output of the PHY. A scan chain structure is added to provide additional coverage of stuck-at and 60MHz at-speed transition faults. To facilitate debugging, a second set of serial pattern generator and data checker is implemented at the digital boundary. The BIST provides a fault coverage of 78.68% and 81.67% for stuck-at and 2.4GHz domain at-speed transition faults respectively for the PHY digital logic, while an ATPG coverage of 99.22% and 94.13% is achieved for stuck-at and 60MHz at-speed transition faults respectively using the scan chain structure. The BIST area and current consumption overhead is seen to be 0.5% and 2.5% respectively, thus providing a low cost test solution.},
author = {Piplani, Surya and Fonseca, Humberto and Sharma, Vivek Mohan and Cervini, Daniele and Hardisty, David},
booktitle = {Proceedings of the Asian Test Symposium},
doi = {10.1109/ATS.2017.43},
isbn = {9781538624364},
issn = {10817735},
keywords = {High Speed Serial Link,JESD204B,Mixed Signal BIST,PHY ATPG},
month = {jan},
pages = {179--183},
publisher = {IEEE Computer Society},
title = {{Test and debug strategy for high speed JESD204B Rx PHY}},
year = {2018}
}
@inproceedings{Goel2013,
abstract = {Recent advances in semiconductor process technology especially interconnects using Through Silicon Vias (TSVs) enable the heterogeneous system integration where dies are implemented in dedicated, optimized process technologies and stacked in a 3D form. TSMC has developed the CoWoS™ (Chip on Wafer on Substrate) process as a design paradigm to assemble silicon interposer-based 3D ICs. To reach quality requirements for volume production, several test challenges related to 3D ICs need to be addressed. This paper describes the test and debug strategy used in designing a CoWoS™ based stacked IC. The 3D design presented in the paper contains three heterogeneous dies (a logic, a DRAM, and a JEDEC Wide-I/O compliant DRAM) stacked on the top of a passive interposer. For passive interposer testing, a novel test methodology called Pretty-Good-Die (PGD) test is presented, while for inter-die test, a novel scalable multi-tower 3D DFT architecture is presented. Silicon results show that most of the test challenges can be solved efficiently if planned properly; and 3D ICs are reality and not a fiction anymore. {\textcopyright} 2013 IEEE.},
author = {Goel, Sandeep Kumar and Adham, Saman and Wang, Min Jer and Chen, Ji Jan and Huang, Tze Chiang and Mehta, Ashok and Lee, Frank and Chickermane, Vivek and Keller, Brion and Valind, Thomas and Mukherjee, Subhasish and Sood, Navdeep and Cho, Jeongho and Lee, Hayden Hyungdong and Choi, Jungi and Kim, Sangdoo},
booktitle = {Proceedings - International Test Conference},
doi = {10.1109/TEST.2013.6651893},
isbn = {9781479908592},
issn = {10893539},
pages = {1--10},
title = {{Test and debug strategy for TSMC CoWoS™ stacking process based heterogeneous 3D IC: A silicon case study}},
year = {2013}
}
@inproceedings{Vermeulen2001,
abstract = {Decreasing feature sizes and increasing customer demand for more functionality have forced design teams to re-use design blocks and application platforms. As a result, re-use of test, design-for-test and design-for-debug for large system chips is becoming increasingly important and increasingly necessary. In this paper, the test and debug features of the NexperiaTM PNX8525 chip are presented. The PNX8525 chip is a large system chip for the consumer electronics market. The impact of corebased testing is discussed, at both the core-level and the top-level, together with the design-for-debug implementation on this multiple clock domain chip.},
author = {Vermeulen, Bart and Oostdijk, Steven and Bouwman, Frank},
booktitle = {IEEE International Test Conference (TC)},
doi = {10.1109/test.2001.966625},
issn = {10893539},
pages = {121--130},
title = {{Test and debug strategy of the PNX8525 NexperiaTM digital video platform system chip}},
year = {2001}
}
@article{McDonald2020,
abstract = {This study compared the results of a usability inspection conducted under two separate conditions: An explicit concurrent think-aloud that required explanations and silent working. 12 student analysts inspected two travel websites thinking-aloud and working in silence to produce a set of problem predictions. Overall, the silent working condition produced more initial predictions, but the think-aloud condition yielded a greater proportion of accurate predictions as revealed by falsification testing. The analysts used a range of problem discovery methods with system searching being favoured by the silent working condition and the more active, goal playing discovery method in the think-aloud condition. Thinking-aloud was also associated with a broader spread of knowledge resources.},
author = {McDonald, Sharon and Cockton, Gilbert and Irons, Alastair},
doi = {10.1145/3397876},
issn = {25730142},
journal = {Proceedings of the ACM on Human-Computer Interaction},
keywords = {evaluation resources,heuristic evaluation,think-aloud,usability inspection},
month = {jun},
number = {EICS},
pages = {88:1--88:22},
publisher = {Association for Computing Machinery},
title = {{The Impact of Thinking-Aloud on Usability Inspection}},
volume = {4},
year = {2020}
}
@inproceedings{Gopstein2020,
abstract = {Atoms of confusion are small patterns of code that have been empirically validated to be difficult to hand-evaluate by programmers. Previous research focused on defining and quantifying this phenomenon, but not on explaining or critiquing it. In this work, we address core omissions to the body of work on atoms of confusion, focusing on the 'how' and 'why' of programmer misunderstanding. We performed a think-aloud study in which we observed programmers, both professionals and students, as they hand-evaluated confusing code. We performed a qualitative analysis of the data and found several surprising results, which explain previous results, outline avenues of further research, and suggest improvements of the research methodology. A notable observation is that correct hand-evaluations do not imply understanding, and incorrect evaluations not misunderstanding. We believe this and other observations may be used to improve future studies and models of program comprehension. We argue that thinking of confusion as an atomic construct may pose challenges to formulating new candidates for atoms of confusion. Ultimately, we question whether hand-evaluation correctness is, itself, a sufficient instrument to study program comprehension.},
author = {Gopstein, Dan and Fayard, Anne Laure and Apel, Sven and Cappos, Justin},
booktitle = {ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
doi = {10.1145/3368089.3409714},
isbn = {9781450370431},
keywords = {Atoms of Confusion,Program Understanding,Think-Aloud Study},
month = {nov},
pages = {605--616},
publisher = {Association for Computing Machinery, Inc},
title = {{Thinking aloud about confusing code: A qualitative investigation of program comprehension and atoms of confusion}},
year = {2020}
}
@article{Betz2021,
abstract = {Thinking aloud is an effective meta-cognitive strategy human reasoners apply to solve difficult problems. We suggest to improve the reasoning ability of pre-trained neural language models in a similar way, namely by expanding a task's context with problem elaborations that are dynamically generated by the language model itself. Our main result is that dynamic problem elaboration significantly improves the zero-shot performance of GPT-2 in a deductive reasoning and natural language inference task: While the model uses a syntactic heuristic for predicting an answer, it is capable (to some degree) of generating reasoned additional context which facilitates the successful application of its heuristic. We explore different ways of generating elaborations, including fewshot learning, and find that their relative performance varies with the specific problem characteristics (such as problem difficulty). Moreover, the effectiveness of an elaboration can be explained in terms of the degree to which the elaboration semantically coheres with the corresponding problem. In particular, elaborations that are most faithful to the original problem description may boost accuracy by up to 24%.},
archivePrefix = {arXiv},
arxivId = {2103.13033},
author = {Betz, Gregor and Richardson, Kyle and Voigt, Christian},
eprint = {2103.13033},
journal = {CoRR},
title = {{Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2}},
url = {https://arxiv.org/abs/2103.13033 http://arxiv.org/abs/2103.13033},
volume = {abs/2103.1},
year = {2021}
}
@article{Fan2020a,
abstract = {Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML's input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind).},
author = {Fan, Mingming and Wu, Ke and Zhao, Jian and Li, Yue and Wei, Winter and Truong, Khai N.},
doi = {10.1109/TVCG.2019.2934797},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Think-aloud,UX practices,machine intelligence,session review behavior,usability problems,user study,visual analytics},
month = {jan},
number = {1},
pages = {343--352},
pmid = {31443019},
publisher = {IEEE Computer Society},
title = {{VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions}},
volume = {26},
year = {2020}
}
@article{Liu2021,
abstract = {This article describes a mechanism that enables debug engineers to extract and visualize the logical correlation among events and messages in system-level trace data. It enables debug engineers to focus only on trace packets that are logically correlated to the issue under debug.},
author = {Liu, Chen},
doi = {10.1109/MC.2020.2984507},
issn = {15580814},
journal = {Computer},
month = {mar},
number = {3},
pages = {28--36},
publisher = {IEEE Computer Society},
title = {{Visualizing Logical Correlation in Trace Data for System Debugging}},
volume = {54},
year = {2021}
}
@article{Noyori2021,
abstract = {Resource limitations require that bugs be resolved efficiently. The bug modification process uses bug reports, which are generated from service user reports. Developers read these reports and fix bugs. Developers discuss bugs by posting comments directly in bug reports. Although several studies have investigated the initial report in bug reports, few have researched the comments. Our research focuses on bug reports. Currently, everyone is free to comment, but the bug fixing time may be affected by how to comment. Herein we investigate the topic of comments in bug reports. Mixed topics do not affect the bug fixing time. However, the bug fixing time tends to be shorter when the discussion length of the phenomenon is short.},
author = {Noyori, Yuki and Washizaki, Hironori and Fukazawa, Yoshiaki and Kanuka, Hideyuki and Ooshima, Keishi and Nojiri, Shuhei and Tsuchiya, Ryosuke},
doi = {10.1587/transinf.2020MPP0007},
file = {::},
issn = {17451361},
journal = {IEICE Transactions on Information and Systems},
keywords = {Bug report,Discussion,Pattern mining,Text mining},
month = {jan},
number = {1},
pages = {106--116},
publisher = {Institute of Electronics, Information and Communication, Engineers, IEICE},
title = {{What are the features of good discussions for shortening bug fixing time?}},
volume = {E104D},
year = {2021}
}
@inproceedings{Drew2018,
abstract = {The System Usability Scale (SUS) is widely used as a quick method for measuring usability; however, past research showed there is only a weak relationship between SUS scores and one behavioral usability measure, and alternatively, SUS corresponds more strongly with user preference. This suggests that the underlying constructs of the SUS may not be well understood. In this study, participants were asked to think aloud while completing a usability test and filling out the SUS. Correlations showed no relationship between behavioral performance and SUS scores. Instead, a relationship was observed between SUS scores and perceived success. Furthermore, participants described a variety of reasons for selecting their SUS responses that were unrelated to the usability of the system, which we have termed rationalizations. This suggests that the SUS is constructed of a combination of experiential components, including attitudinal perceptions. Consequently, SUS scores may be more helpful as a tool for comparison (between competitors, iterations, etc.,) or when used in conjunction with formative usability testing methods to provide a holistic view of real and perceived user experience.},
author = {Drew, Mandy R. and Falcone, Brooke and Baccus, Wendy L.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-91797-9_25},
isbn = {9783319917962},
issn = {16113349},
keywords = {SUS,System Usability Scale,Think-aloud protocol,Usability testing,User experience},
pages = {356--366},
publisher = {Springer Verlag},
title = {{What does the system usability scale (SUS) measure?: Validation using think aloud verbalization and behavioral metrics}},
volume = {10918 LNCS},
year = {2018}
}
@article{Hirsch2021,
abstract = {Researchers have developed numerous debugging approaches to help programmers in the debugging process, but these approaches are rarely used in practice. In this paper, we investigate how programmers debug their code and what researchers should consider when developing debugging approaches. We conducted an online questionnaire where 102 programmers provided information about recently fixed bugs. We found that the majority of bugs (69.6 %) are semantic bugs. Memory and concurrency bugs do not occur as frequently (6.9 % and 8.8 %), but they consume more debugging time. Locating a bug is more difficult than reproducing and fixing it. Programmers often use only IDE build-in tools for debugging. Furthermore, programmers frequently use a replication-observation-deduction pattern when debugging. These results suggest that debugging support is particularly valuable for memory and concurrency bugs. Furthermore, researchers should focus on the fault localization phase and integrate their tools into commonly used IDEs.},
archivePrefix = {arXiv},
arxivId = {2103.12447},
author = {Hirsch, Thomas and Hofer, Birgit},
eprint = {2103.12447},
journal = {SER-IP, ICSE comp. to appear},
month = {mar},
pages = {4},
title = {{What we can learn from how programmers debug their code}},
url = {https://arxiv.org/abs/2103.12447 http://arxiv.org/abs/2103.12447},
volume = {abs/2103.1},
year = {2021}
}
@inproceedings{DiLuna2021,
abstract = {Despite the advancements in software testing, bugs still plague deployed software and result in crashes in production. When debugging issues-sometimes caused by "heisenbugs"-there is the need to interpret core dumps and reproduce the issue offline on the same binary deployed. This requires the entire toolchain (compiler, linker, debugger) to correctly generate and use debug information. Little attention has been devoted to checking that such information is correctly preserved by modern toolchains' optimization stages. This is particularly important as managing debug information in optimized production binaries is non-trivial, often leading to toolchain bugs that may hinder post-deployment debugging efforts. In this paper, we present Debug2, a framework to find debug information bugs in modern toolchains. Our framework feeds random source programs to the target toolchain and surgically compares the debugging behavior of their optimized/unoptimized binary variants. Such differential analysis allows Debug2 to check invariants at each debugging step and detect bugs from invariant violations. Our invariants are based on the (in)consistency of common debug entities, such as source lines, stack frames, and function arguments. We show that, while simple, this strategy yields powerful cross-toolchain and cross-language invariants, which can pinpoint several bugs in modern toolchains. We have used Debug2 to find 23 bugs in the LLVM toolchain (clang/lldb), 8 bugs in the GNU toolchain (GCC/gdb), and 3 in the Rust toolchain (rustc/lldb)-with 14 bugs already fixed by the developers.},
archivePrefix = {arXiv},
arxivId = {2011.13994},
author = {{Di Luna}, Giuseppe Antonio and Italiano, Davide and Massarelli, Luca and {\"{O}}sterlund, Sebastian and Giuffrida, Cristiano and Querzoni, Leonardo},
booktitle = {International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS},
doi = {10.1145/3445814.3446695},
eprint = {2011.13994},
isbn = {9781450383172},
keywords = {Debug Information,Optimized Binaries,Verification},
month = {apr},
pages = {1034--1045},
publisher = {Association for Computing Machinery},
title = {{Who's Debugging the Debuggers? Exposing Debug Information Bugs in Optimized Binaries}},
year = {2021}
}
@article{May2019,
abstract = {Background: Recordings of gamers interacting with video games have become a mainstay of online video-sharing communities such as YouTube. Sometimes called Let's Play videos, those recordings include content relatable to usability testing sessions and potentially illustrate basic think-aloud protocols. Literature review: Research regarding think-aloud protocols indicates that the use of video to review concurrent user commentary is a valid usability testing technique, including sessions that include little to no tester instruction or intervention. Evaluation using a heuristic created for the studied interface can support this type of usability testing. Research questions: 1. Based on a heuristic created from video game usability research, do Let's Play videos provide content representative of think-aloud protocols regarding usability of the games played? 2. Are relevant Let's Play videos potentially useful tools for illustrating think-aloud protocols to students unfamiliar with this type of usability testing? Methods: After reviewing research concerning video game heuristics to create a common set of guidelines, the author selected and reviewed five YouTube videos, gathering and coding information related to the heuristic. Results: The recordings were found to contain relevant information regarding video game usability based on the criteria developed from the literature, specifically considering verbalizations relative to think-aloud protocols. Conclusion: Because these gaming videos contain commentary measurable against a research-based heuristic for game usability, they could be used as an additional method to introduce think-aloud protocols to usability students.},
author = {May, Jamie},
doi = {10.1109/TPC.2018.2867130},
issn = {03611434},
journal = {IEEE Transactions on Professional Communication},
keywords = {Heuristic analysis,Let's Play (LP) videos,think-aloud protocols (TAPs),usability testing,video games},
month = {mar},
number = {1},
pages = {94--103},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{YouTube Gamers and Think-Aloud Protocols: Introducing Usability Testing}},
volume = {62},
year = {2019}
}
